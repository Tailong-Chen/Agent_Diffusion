"""
DDPMè®­ç»ƒè„šæœ¬ï¼ˆçº¯FP32ç¨³å®šç‰ˆï¼Œæ— AutoEncoderï¼‰
ä¿®æ”¹å†…å®¹ï¼š
1. ç§»é™¤äº†æ··åˆç²¾åº¦(autocast)ï¼Œè§£å†³è®­ç»ƒåæœŸç”Ÿæˆå…¨é»‘/NaNçš„é—®é¢˜ã€‚
2. æ¯10ä¸ªepochè‡ªåŠ¨ä¿å­˜æ¨¡å‹ã€‚
3. é‡‡æ ·æ—¶å¢åŠ æ•°å€¼èŒƒå›´æ‰“å°ï¼Œä¾¿äºç›‘æ§ã€‚

ä½¿ç”¨æ–¹æ³•:
    python train_ddpm_simple.py --tiff_path ./data/mt.tif --output_dir ./output_ddpm_256 --image_size 256 --batch_size 4
"""

import argparse
import numpy as np
import torch
import torch.nn.functional as F
# ç§»é™¤ AMP ç›¸å…³çš„å¼•ç”¨ï¼Œç¡®ä¿æ•°å€¼ç¨³å®š
# from torch.cuda.amp import autocast, GradScaler 
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import tifffile

from monai import transforms
from monai.utils import set_determinism
from monai.data import CacheDataset, DataLoader

from generative.inferers import DiffusionInferer
from generative.networks.nets import DiffusionModelUNet
from generative.networks.schedulers import DDPMScheduler


class TiffDataset:
    """TIFFæ•°æ®é›†"""
    
    def __init__(self, images_array, transform=None):
        self.data = [{"image": img} for img in images_array]
        self.transform = transform
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        image = self.data[idx]["image"].astype(np.float32)
        if image.ndim == 2:
            image = image[np.newaxis, ...]
        data_dict = {"image": image}
        if self.transform:
            data_dict = self.transform(data_dict)
        return data_dict


def train_ddpm(
    tiff_path,
    output_dir,
    image_size=256,
    n_epochs=150,
    batch_size=4,
    val_interval=200,
    sample_interval=200, # å»ºè®®è®¾ç½®å°ä¸€ç‚¹ï¼Œæ¯”å¦‚ 20æˆ–50ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
    num_inference_steps=1000,
    seed=42,
    resume_from=None,
    load_optimizer=False,
):
    """
    DDPMè®­ç»ƒä¸»å‡½æ•° (FP32ç¨³å®šç‰ˆ)
    """
    
    # ============================================
    # 1. è®¾ç½®
    # ============================================
    set_determinism(seed)
    device = torch.device("cuda:2" if torch.cuda.is_available() else "cpu")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    (output_dir / "checkpoints").mkdir(exist_ok=True)
    (output_dir / "samples").mkdir(exist_ok=True)
    
    print("="*70)
    print("ğŸš€ DDPMè®­ç»ƒï¼ˆFP32ç¨³å®šç‰ˆ - è§£å†³çº¯é»‘Bugï¼‰")
    print("="*70)
    print(f"å›¾åƒå°ºå¯¸: {image_size}Ã—{image_size}")
    print(f"Batch Size: {batch_size}")
    print(f"Epochs: {n_epochs}")
    print(f"æ¯10ä¸ªEpochä¿å­˜ä¸€æ¬¡æ¨¡å‹")
    
    # ============================================
    # 2. åŠ è½½æ•°æ®
    # ============================================
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    all_images = tifffile.imread(tiff_path)
    if all_images.ndim == 2:
        all_images = all_images[np.newaxis, ...]
    
    print(f"æ€»å›¾åƒæ•°: {len(all_images)}")
    
    # ç¡®å®šå½’ä¸€åŒ–èŒƒå›´
    if all_images.dtype == np.uint8:
        scale_max = 255.0
    elif all_images.dtype == np.uint16:
        scale_max = 65535.0
    else:
        scale_max = float(all_images.max())
    
    print(f"æ•°æ®èŒƒå›´: [0, {scale_max}]")
    
    # åˆ’åˆ†æ•°æ®é›†
    train_size = int(0.9 * len(all_images))
    indices = np.random.permutation(len(all_images))
    train_images = all_images[indices[:train_size]]
    val_images = all_images[indices[train_size:]]
    
    # ============================================
    # 3. æ•°æ®å˜æ¢
    # ============================================
    train_transforms = transforms.Compose([
        transforms.EnsureChannelFirstd(keys=["image"], channel_dim="no_channel"),
        transforms.ScaleIntensityRanged(
            keys=["image"], a_min=0.0, a_max=scale_max,
            b_min=0.0, b_max=1.0, clip=True
        ),
        transforms.RandAffined(
            keys=["image"],
            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],
            translate_range=[(-1, 1), (-1, 1)],
            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],
            spatial_size=[image_size, image_size],
            padding_mode="zeros",
            prob=0.5,
        ),
    ])
    
    val_transforms = transforms.Compose([
        transforms.EnsureChannelFirstd(keys=["image"], channel_dim="no_channel"),
        transforms.ScaleIntensityRanged(
            keys=["image"], a_min=0.0, a_max=scale_max,
            b_min=0.0, b_max=1.0, clip=True
        ),
        transforms.Resized(
            keys=["image"],
            spatial_size=[image_size, image_size],
        ),
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_ds = CacheDataset(
        data=TiffDataset(train_images, transform=None).data,
        transform=train_transforms
    )
    val_ds = CacheDataset(
        data=TiffDataset(val_images, transform=None).data,
        transform=val_transforms
    )
    
    # DataLoader
    train_loader = DataLoader(
        train_ds, batch_size=batch_size, shuffle=True,
        num_workers=4, persistent_workers=True
    )
    val_loader = DataLoader(
        val_ds, batch_size=batch_size, shuffle=False,
        num_workers=4, persistent_workers=True
    )
    
    # ============================================
    # 4. åˆå§‹åŒ–æ¨¡å‹
    # ============================================
    print("\nğŸ”§ åˆå§‹åŒ–UNet...")
    
    if image_size >= 512:
        num_channels = (128, 256, 512)
        attention_levels = (False, False, False)
        num_head_channels = 512
    else:
        num_channels = (128, 256, 256)
        attention_levels = (False, True, True)
        num_head_channels = 256
    
    unet = DiffusionModelUNet(
        spatial_dims=2,
        in_channels=1,
        out_channels=1,
        num_res_blocks=2,
        num_channels=num_channels,
        attention_levels=attention_levels,
        num_head_channels=num_head_channels,
    ).to(device)
    
    model_params = sum(p.numel() for p in unet.parameters()) / 1e6
    print(f"âœ… UNetå‚æ•°é‡: {model_params:.2f}M")
    
    # Schedulerå’ŒInferer
    scheduler = DDPMScheduler(num_train_timesteps=1000)
    inferer = DiffusionInferer(scheduler)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(params=unet.parameters(), lr=1e-4)
    
    # âš ï¸ å·²ç§»é™¤ GradScalerï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨ä½¿ç”¨çº¯ FP32 è®­ç»ƒ
    
    start_epoch = 0

    if resume_from is not None:
        ckpt_path = Path(resume_from)
        if ckpt_path.exists():
            print(f"ğŸ” åŠ è½½checkpoint: {ckpt_path}")
            ckpt = torch.load(str(ckpt_path), map_location=device)
            
            if 'model_state_dict' in ckpt:
                unet.load_state_dict(ckpt['model_state_dict'])
                print("   âœ… æ¨¡å‹æƒé‡å·²åŠ è½½")
            
            if load_optimizer and 'optimizer_state_dict' in ckpt:
                try:
                    optimizer.load_state_dict(ckpt['optimizer_state_dict'])
                    print("   âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
                except Exception as e:
                    print(f"   âŒ æ— æ³•åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€: {e}")

            start_epoch = int(ckpt.get('epoch', 0))
            print(f"   â–¶ï¸ ä» epoch {start_epoch+1} å¼€å§‹ç»§ç»­è®­ç»ƒ")
        else:
            print(f"âš ï¸ æŒ‡å®šçš„ checkpoint ä¸å­˜åœ¨: {ckpt_path}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # ============================================
    # 5. è®­ç»ƒå¾ªç¯
    # ============================================
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ (FP32æ¨¡å¼)...\n")
    
    best_val_loss = float('inf')
    epoch_losses = []
    val_losses = []
    
    for epoch in range(start_epoch, n_epochs):
        # ========== è®­ç»ƒ ==========
        unet.train()
        epoch_loss = 0
        progress_bar = tqdm(
            train_loader,
            desc=f"Epoch {epoch+1}/{n_epochs}",
            ncols=110
        )
        
        for batch in progress_bar:
            images = batch["image"].to(device)
            
            # ç¡®ä¿æ˜¯ float32
            images = images.float()
            
            optimizer.zero_grad(set_to_none=True)
            
            # âŒ ç§»é™¤ autocast
            # with autocast(enabled=True):
            
            # ç”Ÿæˆå™ªå£°
            noise = torch.randn_like(images).to(device)
            
            timesteps = torch.randint(
                0, inferer.scheduler.num_train_timesteps,
                (images.shape[0],),
                device=device
            ).long()
            
            # å‰å‘ä¼ æ’­
            noise_pred = inferer(
                inputs=images,
                diffusion_model=unet,
                noise=noise,
                timesteps=timesteps
            )
            
            loss = F.mse_loss(noise_pred, noise)
            
            # âŒ ç§»é™¤ scalerï¼Œä½¿ç”¨æ ‡å‡†åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            # scaler.update()
            
            epoch_loss += loss.item()
            
            n_batches = progress_bar.n if progress_bar.n > 0 else 1
            progress_bar.set_postfix({"loss": f"{epoch_loss/n_batches:.5f}"})
        
        avg_train_loss = epoch_loss / len(train_loader)
        epoch_losses.append(avg_train_loss)
        
        # ========== éªŒè¯ ==========
        if (epoch + 1) % val_interval == 0:
            unet.eval()
            val_loss = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    images = batch["image"].to(device).float() # ç¡®ä¿ float32
                    
                    noise = torch.randn_like(images).to(device)
                    timesteps = torch.randint(
                        0, inferer.scheduler.num_train_timesteps,
                        (images.shape[0],),
                        device=device
                    ).long()
                    
                    noise_pred = inferer(
                        inputs=images,
                        diffusion_model=unet,
                        noise=noise,
                        timesteps=timesteps
                    )
                    loss = F.mse_loss(noise_pred, noise)
                    val_loss += loss.item()
            
            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)
            
            print(f"\nğŸ“Š Epoch {epoch+1} | Train: {avg_train_loss:.5f} | Val: {avg_val_loss:.5f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save({
                    "epoch": epoch + 1,
                    "model_state_dict": unet.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "train_loss": avg_train_loss,
                    "val_loss": avg_val_loss,
                }, output_dir / "checkpoints" / "best_model.pth")
                print("ğŸ’¾ Best model saved!")

        # ========== æ¯10ä¸ªEpochä¿å­˜ä¸€æ¬¡æ¨¡å‹ (æ–°å¢) ==========
        if (epoch + 1) % 10 == 0:
            save_path = output_dir / "checkpoints" / f"model_epoch_{epoch+1}.pth"
            torch.save({
                "epoch": epoch + 1,
                "model_state_dict": unet.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "train_loss": avg_train_loss,
            }, save_path)
            print(f"ğŸ’¾ Checkpoint saved: {save_path.name}")

        # ========== ç”Ÿæˆæ ·æœ¬ï¼ˆå¸¦è¿›åº¦æ¡ + æ•°å€¼ç›‘æ§ï¼‰ ==========
        if (epoch + 1) % sample_interval == 0:
            print(f"   ğŸ¨ ç”Ÿæˆæ ·æœ¬ä¸­ï¼ˆ{num_inference_steps}æ­¥ï¼‰...")
            unet.eval()
            scheduler.set_timesteps(num_inference_steps=num_inference_steps)
            
            with torch.no_grad():
                # 1. åˆå§‹åŒ–å™ªå£° (ç¡®ä¿æ˜¯float32)
                noise = torch.randn((1, 1, image_size, image_size)).to(device).float()
                image = noise
                
                progress_sampling = tqdm(
                    scheduler.timesteps,
                    desc="   Sampling",
                    ncols=110,
                    leave=False
                )
                
                # âŒ å¿…é¡»ç§»é™¤ autocastï¼Œå¦åˆ™é‡‡æ ·å®¹æ˜“å˜æˆå…¨é»‘
                for t in progress_sampling:
                    model_output = unet(
                        x=image,
                        timesteps=torch.Tensor((t,)).to(device).long()
                    )
                    step_result = scheduler.step(model_output, t, image)
                    
                    if isinstance(step_result, tuple):
                        image = step_result[0]
                    else:
                        image = step_result.prev_sample
            
            # 2. æ•°å€¼ç›‘æ§ (æ‰“å°å‡ºæ¥çœ‹çœ‹æ˜¯ä¸æ˜¯å…¨æ˜¯0æˆ–è€…Nan)
            d_min, d_max, d_mean = image.min().item(), image.max().item(), image.mean().item()
            print(f"   ğŸ” æ ·æœ¬ç»Ÿè®¡ - Min: {d_min:.4f} | Max: {d_max:.4f} | Mean: {d_mean:.4f}")
            
            if d_max == 0 and d_min == 0:
                print("   âŒ è­¦å‘Šï¼šç”Ÿæˆçš„å›¾åƒæ˜¯çº¯é»‘ï¼ˆå…¨0ï¼‰ï¼")
            if np.isnan(d_mean):
                print("   âŒ è­¦å‘Šï¼šç”Ÿæˆçš„å›¾åƒåŒ…å« NaNï¼")

            # ä¿å­˜æ ·æœ¬
            plt.figure(figsize=(6, 6))
            plt.imshow(image[0, 0].cpu().numpy(), vmin=0, vmax=1, cmap="gray")
            plt.title(f"Epoch {epoch+1} (Max:{d_max:.2f})")
            plt.axis("off")
            plt.tight_layout()
            plt.savefig(
                output_dir / "samples" / f"sample_epoch_{epoch+1}.png",
                dpi=150, bbox_inches='tight'
            )
            plt.close()
            print(f"   âœ… æ ·æœ¬å·²ä¿å­˜\n")
    
    # ============================================
    # 6. è®­ç»ƒå®Œæˆåçš„å¤„ç†
    # ============================================
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    plt.figure(figsize=(10, 5))
    plt.plot(epoch_losses, label="Train Loss", linewidth=2)
    if val_losses:
        val_epochs = np.arange(val_interval - 1, n_epochs, val_interval)
        plt.plot(val_epochs, val_losses, label="Val Loss", linewidth=2)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(output_dir / "learning_curves.png", dpi=150)
    plt.close()
    
    # ç”Ÿæˆæœ€ç»ˆæ ·æœ¬ç½‘æ ¼
    print("\nğŸ¨ ç”Ÿæˆæœ€ç»ˆæ ·æœ¬ç½‘æ ¼ï¼ˆ8å¼ ï¼‰...")
    unet.eval()
    scheduler.set_timesteps(num_inference_steps=num_inference_steps)
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.flatten()
    
    with torch.no_grad():
        for i in range(8):
            noise = torch.randn((1, 1, image_size, image_size)).to(device).float()
            image = noise
            
            progress_sampling = tqdm(
                scheduler.timesteps,
                desc=f"   Sample {i+1}",
                ncols=110,
                leave=False
            )
            
            for t in progress_sampling:
                model_output = unet(
                    x=image,
                    timesteps=torch.Tensor((t,)).to(device).long()
                )
                step_result = scheduler.step(model_output, t, image)
                if isinstance(step_result, tuple):
                    image = step_result[0]
                else:
                    image = step_result.prev_sample
            
            axes[i].imshow(image[0, 0].cpu().numpy(), vmin=0, vmax=1, cmap="gray")
            axes[i].set_title(f"Sample {i+1}")
            axes[i].axis("off")
    
    plt.tight_layout()
    plt.savefig(output_dir / "final_samples.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å…¨éƒ¨å®Œæˆ: {output_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç®€å•DDPMè®­ç»ƒï¼ˆç¨³å®šç‰ˆï¼Œæ— AMPï¼‰")
    parser.add_argument("--tiff_path", type=str, required=True, help="TIFFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./output_ddpm", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--image_size", type=int, default=256, help="å›¾åƒå°ºå¯¸")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--n_epochs", type=int, default=150, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--val_interval", type=int, default=200,help="éªŒè¯é—´éš”")
    parser.add_argument("--sample_interval", type=int, default=10, help="é‡‡æ ·é—´éš”ï¼ˆæ¨èè®¾ç½®å°ä¸€ç‚¹ä»¥ç›‘æ§ï¼‰")
    parser.add_argument("--num_inference_steps", type=int, default=1000, help="é‡‡æ ·æ­¥æ•°")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--resume_from", type=str, default=None, help="checkpointè·¯å¾„")
    parser.add_argument("--load_optimizer", action="store_true", help="æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨")
    
    args = parser.parse_args()
    
    train_ddpm(
        tiff_path=args.tiff_path,
        output_dir=args.output_dir,
        image_size=args.image_size,
        n_epochs=args.n_epochs,
        batch_size=args.batch_size,
        val_interval=args.val_interval,
        sample_interval=args.sample_interval,
        num_inference_steps=args.num_inference_steps,
        seed=args.seed,
        resume_from=args.resume_from,
        load_optimizer=args.load_optimizer,
    )