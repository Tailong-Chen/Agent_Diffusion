# é«˜åˆ†è¾¨ç‡å›¾åƒç”ŸæˆæŒ‡å— (32GBæ˜¾å­˜)

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨32GBæ˜¾å­˜è®­ç»ƒé«˜åˆ†è¾¨ç‡ï¼ˆ512Ã—512 æˆ– 1024Ã—1024ï¼‰çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚

---

## ğŸ¯ æ¨èæ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | 512Ã—512 â­ | 1024Ã—1024 |
|------|-----------|-----------|
| **å¯è¡Œæ€§** | âœ… å¼ºçƒˆæ¨è | âš ï¸ éœ€è¦å¤§é‡ä¼˜åŒ– |
| **æ‰¹æ¬¡å¤§å°** | 4-8 | 1-2 |
| **è®­ç»ƒæ—¶é—´** | 10-15å°æ—¶ | 2-4å¤© |
| **è®­ç»ƒç¨³å®šæ€§** | é«˜ | ä¸­ç­‰ |
| **æ˜¾å­˜åˆ©ç”¨ç‡** | 60-80% | 90-100% |
| **ç»“æœè´¨é‡** | ä¼˜ç§€ | ä¼˜ç§€ï¼ˆå¦‚æœè®­ç»ƒå……åˆ†ï¼‰ |

**ç»“è®º**: å¯¹äº32GBæ˜¾å­˜ï¼Œ**512Ã—512æ˜¯æœ€ä½³é€‰æ‹©**ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ¡ˆA: 512Ã—512 (æ¨è)

#### 1. ä¿®æ”¹åŸæ•™ç¨‹é…ç½®

åœ¨ `2d_ldm_tutorial.py` ä¸­ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼š

```python
# ç¬¬90è¡Œé™„è¿‘ - ä¿®æ”¹å›¾åƒå°ºå¯¸
image_size = 512  # åŸæ¥æ˜¯64

# ç¬¬108è¡Œé™„è¿‘ - è°ƒæ•´æ‰¹æ¬¡å¤§å°
train_loader = DataLoader(train_ds, batch_size=6, shuffle=True, num_workers=4, persistent_workers=True)

# ç¬¬132è¡Œé™„è¿‘ - éªŒè¯é›†æ‰¹æ¬¡å¤§å°
val_loader = DataLoader(val_ds, batch_size=6, shuffle=True, num_workers=4, persistent_workers=True)

# ç¬¬143-153è¡Œ - ä¿®æ”¹AutoencoderKLé…ç½®
autoencoderkl = AutoencoderKL(
    spatial_dims=2,
    in_channels=1,
    out_channels=1,
    num_channels=(128, 256, 512),  # 3å±‚ä¸‹é‡‡æ ·: 512â†’256â†’128â†’64
    latent_channels=4,              # å¢åŠ åˆ°4
    num_res_blocks=2,
    attention_levels=(False, False, True),  # æœ€é«˜å±‚å¯ç”¨æ³¨æ„åŠ›
    with_encoder_nonlocal_attn=False,
    with_decoder_nonlocal_attn=False,
)

# ç¬¬184è¡Œé™„è¿‘ - å¢åŠ è®­ç»ƒè½®æ•°
n_epochs = 150  # åŸæ¥æ˜¯100

# ç¬¬303-311è¡Œ - ä¿®æ”¹DiffusionModelUNeté…ç½®
unet = DiffusionModelUNet(
    spatial_dims=2,
    in_channels=4,        # åŒ¹é…latent_channels
    out_channels=4,
    num_res_blocks=2,
    num_channels=(128, 256, 512, 768),  # å¢åŠ å®¹é‡
    attention_levels=(False, True, True, True),
    num_head_channels=(0, 256, 512, 768),
)

# ç¬¬344è¡Œé™„è¿‘ - å¢åŠ æ‰©æ•£æ¨¡å‹è®­ç»ƒè½®æ•°
n_epochs = 250  # åŸæ¥æ˜¯200

# ç¬¬409è¡Œé™„è¿‘ - è°ƒæ•´æ½œåœ¨ç©ºé—´é‡‡æ ·å°ºå¯¸
# 512Ã—512å›¾åƒï¼Œ3å±‚ä¸‹é‡‡æ ·åæ˜¯64Ã—64
z = torch.randn((1, 4, 64, 64))  # åŸæ¥æ˜¯(1, 3, 16, 16)

# ç¬¬449è¡Œé™„è¿‘ - æ¨ç†æ—¶åŒæ ·è°ƒæ•´
noise = torch.randn((1, 4, 64, 64))
```

#### 2. ä¿®æ”¹æ•°æ®å˜æ¢

```python
# ç¬¬96-104è¡Œ - è®­ç»ƒæ•°æ®å˜æ¢
transforms.RandAffined(
    keys=["image"],
    rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],
    translate_range=[(-1, 1), (-1, 1)],
    scale_range=[(-0.05, 0.05), (-0.05, 0.05)],
    spatial_size=[512, 512],  # ä¿®æ”¹è¿™é‡Œ
    padding_mode="zeros",
    prob=0.5,
),
```

#### 3. è¿è¡Œè®­ç»ƒ

```bash
# å¦‚æœä½¿ç”¨Jupyter
jupyter notebook 2d_ldm_tutorial.ipynb

# å¦‚æœä½¿ç”¨Pythonè„šæœ¬
python 2d_ldm_tutorial.py
```

---

### æ–¹æ¡ˆB: 1024Ã—1024 (é«˜çº§)

#### âš ï¸ é‡è¦å‰ææ¡ä»¶

1. **å¿…é¡»ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ** (å·²åœ¨åŸæ•™ç¨‹ä¸­ä½¿ç”¨)
2. **å¿…é¡»ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**
3. **å¼ºçƒˆå»ºè®®ä½¿ç”¨æ¸è¿›å¼è®­ç»ƒ** (256â†’512â†’1024)
4. **batch_sizeå¿…é¡»è®¾ä¸º1æˆ–2**
5. **é¢„è®¡è®­ç»ƒæ—¶é—´2-4å¤©**

#### å…³é”®é…ç½®ä¿®æ”¹

```python
# å›¾åƒå°ºå¯¸
image_size = 1024

# æ‰¹æ¬¡å¤§å°ï¼ˆæé™ï¼‰
batch_size = 2

# æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps = 4  # ç­‰æ•ˆbatch_size=8

# AutoencoderKL
autoencoderkl = AutoencoderKL(
    spatial_dims=2,
    in_channels=1,
    out_channels=1,
    num_channels=(128, 256, 512),  # 1024â†’512â†’256â†’128
    latent_channels=4,
    num_res_blocks=2,
    attention_levels=(False, False, True),
    with_encoder_nonlocal_attn=False,
    with_decoder_nonlocal_attn=False,
)

# UNet (æ½œåœ¨ç©ºé—´æ˜¯128Ã—128Ã—4)
unet = DiffusionModelUNet(
    spatial_dims=2,
    in_channels=4,
    out_channels=4,
    num_res_blocks=2,
    num_channels=(128, 256, 512, 768),
    attention_levels=(False, False, True, True),
    num_head_channels=(0, 0, 512, 768),
)

# è®­ç»ƒè½®æ•°
autoencoder_epochs = 200
diffusion_epochs = 300

# æ½œåœ¨ç©ºé—´å°ºå¯¸
z = torch.randn((1, 4, 128, 128))  # 1024Ã·8=128
```

#### æ¢¯åº¦ç´¯ç§¯å®ç°

åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ï¼š

```python
accumulation_steps = 4

for step, batch in progress_bar:
    images = batch["image"].to(device)
    
    with autocast(enabled=True):
        # ... è®¡ç®—æŸå¤±
        loss_g = loss_g / accumulation_steps  # é‡è¦ï¼
    
    scaler_g.scale(loss_g).backward()
    
    # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
    if (step + 1) % accumulation_steps == 0:
        # å¯é€‰ï¼šæ¢¯åº¦è£å‰ª
        scaler_g.unscale_(optimizer_g)
        torch.nn.utils.clip_grad_norm_(autoencoderkl.parameters(), 1.0)
        
        scaler_g.step(optimizer_g)
        scaler_g.update()
        optimizer_g.zero_grad(set_to_none=True)
```

---

## ğŸ’¾ æ˜¾å­˜ä¼˜åŒ–æŠ€å·§æ€»ç»“

### å·²åœ¨åŸæ•™ç¨‹ä¸­ä½¿ç”¨çš„ä¼˜åŒ–
- âœ… æ··åˆç²¾åº¦è®­ç»ƒ (`autocast`)
- âœ… é«˜æ•ˆä¼˜åŒ–å™¨è®¾ç½® (`set_to_none=True`)
- âœ… æ½œåœ¨æ‰©æ•£ (åœ¨ä½ç»´ç©ºé—´è®­ç»ƒ)

### é’ˆå¯¹32GBæ˜¾å­˜çš„é¢å¤–ä¼˜åŒ–

#### 1. æ¢¯åº¦ç´¯ç§¯ â­â­â­
```python
# ä¸å¢åŠ æ˜¾å­˜ï¼Œæ¨¡æ‹Ÿæ›´å¤§çš„batch size
accumulation_steps = 4
loss = loss / accumulation_steps
```

#### 2. é™ä½æ‰¹æ¬¡å¤§å° â­â­â­
```python
# 1024Ã—1024æ—¶
batch_size = 2  # ç”šè‡³å¯èƒ½éœ€è¦1
```

#### 3. å‡å°‘éªŒè¯é¢‘ç‡ â­â­
```python
val_interval = 50  # å¢å¤§é—´éš”
```

#### 4. å®šæœŸæ¸…ç†ç¼“å­˜ â­
```python
if step % 50 == 0:
    torch.cuda.empty_cache()
```

#### 5. æ¢¯åº¦æ£€æŸ¥ç‚¹ â­â­â­ (é«˜çº§)
```python
from torch.utils.checkpoint import checkpoint

# éœ€è¦ä¿®æ”¹æ¨¡å‹forwardå‡½æ•°
# å¯èŠ‚çœ40-50%æ˜¾å­˜ï¼Œä½†å¢åŠ 30%è®­ç»ƒæ—¶é—´
```

#### 6. 8-bitä¼˜åŒ–å™¨ â­â­ (å¯é€‰)
```python
# å®‰è£…: pip install bitsandbytes
import bitsandbytes as bnb
optimizer = bnb.optim.Adam8bit(model.parameters(), lr=1e-4)
```

---

## ğŸ“Š é¢„æœŸæ˜¾å­˜ä½¿ç”¨

### 512Ã—512é…ç½®
- **AutoencoderKLè®­ç»ƒ**: ~18-22 GB
- **DiffusionModelè®­ç»ƒ**: ~20-24 GB
- **æ¨ç†**: ~8-12 GB

### 1024Ã—1024é…ç½®
- **AutoencoderKLè®­ç»ƒ**: ~28-32 GB (æ»¡è½½!)
- **DiffusionModelè®­ç»ƒ**: ~28-30 GB
- **æ¨ç†**: ~15-20 GB

**ç›‘æ§å‘½ä»¤**:
```bash
# å®æ—¶ç›‘æ§æ˜¾å­˜
watch -n 1 nvidia-smi

# Pythonä¸­ç›‘æ§
print(f"å½“å‰æ˜¾å­˜: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
print(f"å³°å€¼æ˜¾å­˜: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB")
```

---

## ğŸ“ æ¸è¿›å¼è®­ç»ƒç­–ç•¥ (å¼ºçƒˆæ¨èç”¨äº1024Ã—1024)

### ä¸ºä»€ä¹ˆä½¿ç”¨æ¸è¿›å¼è®­ç»ƒï¼Ÿ
1. æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹
2. æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦
3. æ›´å¥½çš„æœ€ç»ˆæ•ˆæœ
4. æ›´å°‘çš„æ˜¾å­˜å‹åŠ›

### ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆ

#### é˜¶æ®µ1: 256Ã—256 (åŸºç¡€é˜¶æ®µ)
```python
image_size = 256
batch_size = 16
epochs = 50
# è®­ç»ƒAutoencoderKL + Diffusion
```

#### é˜¶æ®µ2: 512Ã—512 (è¿‡æ¸¡é˜¶æ®µ)
```python
image_size = 512
batch_size = 6
epochs = 100
# åŠ è½½é˜¶æ®µ1çš„æƒé‡ï¼Œç»§ç»­è®­ç»ƒ
```

#### é˜¶æ®µ3: 1024Ã—1024 (æœ€ç»ˆé˜¶æ®µ)
```python
image_size = 1024
batch_size = 2
epochs = 200
# åŠ è½½é˜¶æ®µ2çš„æƒé‡ï¼Œæœ€ç»ˆfine-tune
```

### æƒé‡è¿ç§»
```python
# ä»ä½åˆ†è¾¨ç‡åŠ è½½æƒé‡
checkpoint = torch.load('checkpoint_512.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# ç»§ç»­è®­ç»ƒæ›´é«˜åˆ†è¾¨ç‡
# æ³¨æ„ï¼šæ½œåœ¨ç©ºé—´å°ºå¯¸ä¼šæ”¹å˜ï¼ŒUNetéœ€è¦é€‚é…
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜æº¢å‡º (OOM)
**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°batch_sizeåˆ°2æˆ–1
2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯
3. å‡å°‘num_res_blocksåˆ°1
4. è€ƒè™‘ä½¿ç”¨æ›´å¤šä¸‹é‡‡æ ·å±‚
5. é™ä½num_channels

### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢
**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°‘num_workers
2. ä½¿ç”¨æ›´å°‘çš„éªŒè¯æ­¥éª¤
3. ä¸ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
4. è€ƒè™‘ä»512Ã—512å¼€å§‹

### Q3: ç”Ÿæˆçš„å›¾åƒè´¨é‡ä¸ä½³
**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ è®­ç»ƒè½®æ•°
2. æ£€æŸ¥scaling_factoræ˜¯å¦åˆé€‚
3. è°ƒæ•´å­¦ä¹ ç‡
4. ä½¿ç”¨æ¸è¿›å¼è®­ç»ƒ
5. ç¡®ä¿æ•°æ®è´¨é‡è‰¯å¥½

### Q4: å¦‚ä½•æ¢å¤è®­ç»ƒï¼Ÿ
```python
# ä¿å­˜checkpoint
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}, 'checkpoint.pth')

# åŠ è½½checkpoint
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch'] + 1
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§å»ºè®®

### å…³é”®æŒ‡æ ‡

1. **AutoencoderKLé˜¶æ®µ**:
   - é‡å»ºæŸå¤± (recons_loss): åº”æŒç»­ä¸‹é™
   - ç›®æ ‡: < 0.02 (512Ã—512), < 0.03 (1024Ã—1024)

2. **Diffusioné˜¶æ®µ**:
   - MSEæŸå¤±: åº”æ”¶æ•›åˆ°0.10-0.15
   - æ¯40ä¸ªepochæ£€æŸ¥ç”Ÿæˆè´¨é‡

3. **æ˜¾å­˜ä½¿ç”¨**:
   - 512Ã—512: ä¸åº”è¶…è¿‡28GB
   - 1024Ã—1024: åº”ä¿æŒåœ¨31GBä»¥ä¸‹

### å¯è§†åŒ–å»ºè®®
```python
# å®šæœŸä¿å­˜ç”Ÿæˆæ ·æœ¬
if epoch % 10 == 0:
    with torch.no_grad():
        sample = inferer.sample(...)
        plt.imsave(f'sample_epoch_{epoch}.png', sample[0,0].cpu())
```

---

## âœ… æ¨èå·¥ä½œæµç¨‹

### å¯¹äº512Ã—512:
1. âœ… ç›´æ¥ä½¿ç”¨æœ¬æŒ‡å—çš„é…ç½®
2. âœ… è®­ç»ƒ150 epochs AutoencoderKL (~4å°æ—¶)
3. âœ… è®­ç»ƒ250 epochs DiffusionModel (~6å°æ—¶)
4. âœ… æ€»è®¡çº¦10-12å°æ—¶å³å¯è·å¾—ä¼˜è´¨ç»“æœ

### å¯¹äº1024Ã—1024:
1. âœ… å¼ºçƒˆå»ºè®®ä½¿ç”¨æ¸è¿›å¼è®­ç»ƒ
2. âœ… æˆ–ä½¿ç”¨é¢„è®­ç»ƒçš„512Ã—512æ¨¡å‹fine-tune
3. âœ… å¯†åˆ‡ç›‘æ§æ˜¾å­˜ä½¿ç”¨
4. âœ… ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å’Œå…¶ä»–ä¼˜åŒ–æŠ€å·§
5. âœ… å‡†å¤‡2-4å¤©çš„è®­ç»ƒæ—¶é—´

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [Stable Diffusionè®ºæ–‡](https://arxiv.org/abs/2112.10752)
- [MONAI Generative Modelsæ–‡æ¡£](https://github.com/Project-MONAI/GenerativeModels)
- åŸæ•™ç¨‹: `2d_ldm_tutorial.ipynb`
- é…ç½®æ–‡ä»¶: `config_512.py`, `config_1024_optimized.py`
- è®­ç»ƒç¤ºä¾‹: `train_high_res_example.py`

---

## ğŸ’¡ æœ€ç»ˆå»ºè®®

**å¯¹äºæ‚¨çš„32GBæ˜¾å­˜ç¯å¢ƒ:**

1. **é¦–é€‰512Ã—512**: æœ€ä½³çš„æ•ˆæœ/æˆæœ¬å¹³è¡¡
2. **æ•°æ®è´¨é‡**: ç¡®ä¿è®­ç»ƒæ•°æ®è´¨é‡é«˜ä¸”å……è¶³
3. **ç›‘æ§è®­ç»ƒ**: å®æ—¶ç›‘æ§losså’Œç”Ÿæˆè´¨é‡
4. **ä¿å­˜checkpoint**: æ¯10-20 epochsä¿å­˜ä¸€æ¬¡
5. **è€å¿ƒç­‰å¾…**: é«˜è´¨é‡ç”Ÿæˆéœ€è¦å……åˆ†è®­ç»ƒ

**é¢„æœŸæ•ˆæœ**:
- 512Ã—512: 10-15å°æ—¶è·å¾—ä¼˜è´¨ç»“æœ â­â­â­â­â­
- 1024Ã—1024: 2-4å¤©å¯èƒ½è·å¾—ä¼˜è´¨ç»“æœ â­â­â­

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

