"""
Diffusionæ¨¡å‹ç”Ÿæˆè´¨é‡è¯Šæ–­è„šæœ¬
ç”¨äºæ’æŸ¥LDMç”Ÿæˆæ•ˆæœå·®çš„é—®é¢˜

ä½¿ç”¨æ–¹æ³•:
    python debug_diffusion.py --checkpoint ./output_ldm/checkpoints/diffusion_epoch_XXX.pth --tiff_path your_data.tif
"""

import argparse
import numpy as np
import torch
import torch.nn.functional as F
from torch.cuda.amp import autocast
import matplotlib.pyplot as plt
from pathlib import Path
import tifffile

from monai import transforms
from torch.utils.data import DataLoader

from generative.inferers import LatentDiffusionInferer
from generative.networks.nets import AutoencoderKL, DiffusionModelUNet
from generative.networks.schedulers import DDPMScheduler

# å¤ç”¨train_tiff_ldm.pyçš„æ•°æ®é›†ç±»
import sys
sys.path.append(str(Path(__file__).parent))
from train_tiff_ldm import TiffStackDataset


class DiffusionDebugger:
    """LDMè¯Šæ–­å·¥å…·"""
    
    def __init__(self, checkpoint_path, tiff_path, device="cuda"):
        self.device = torch.device(device)
        self.checkpoint_path = checkpoint_path
        self.tiff_path = tiff_path
        
        # åŠ è½½checkpoint
        print("="*60)
        print("ğŸ“¦ åŠ è½½Checkpoint...")
        print("="*60)
        self.checkpoint = torch.load(checkpoint_path, map_location=device)
        self.config_dict = self.checkpoint["config"]
        
        # è¯†åˆ«checkpointç±»å‹
        self.checkpoint_stage = self.checkpoint.get("stage", "unknown")
        print(f"ğŸ“‹ Checkpointç±»å‹: {self.checkpoint_stage}")
        
        # é‡å»ºé…ç½®
        self._rebuild_config()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        print(f"âœ… CheckpointåŠ è½½æˆåŠŸ")
        print(f"   - Epoch: {self.checkpoint['epoch'] + 1}")
        print(f"   - Loss: {self.checkpoint.get('loss', 'N/A')}")
        if self.checkpoint_stage == "diffusion":
            print(f"   - Scale Factor: {self.scale_factor:.4f}")
    
    def _rebuild_config(self):
        """ä»checkpointé‡å»ºé…ç½®"""
        self.image_size = self.config_dict["image_size"]
        self.autoencoder_config = self.config_dict["autoencoder_config"]
        self.latent_size = self.config_dict["latent_size"]
        
        # Diffusionç›¸å…³é…ç½®ï¼ˆä»…åœ¨diffusion checkpointä¸­å­˜åœ¨ï¼‰
        if self.checkpoint_stage == "diffusion":
            self.unet_config = self.config_dict["unet_config"]
            self.scheduler_config = self.config_dict["scheduler_config"]
            self.scale_factor = self.checkpoint.get("scale_factor", 1.0)
        else:
            self.unet_config = None
            self.scheduler_config = self.config_dict.get("scheduler_config")
            self.scale_factor = self.checkpoint.get("scale_factor", None)
    
    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        # AutoEncoderï¼ˆæ€»æ˜¯éœ€è¦ï¼‰
        self.autoencoder = AutoencoderKL(**self.autoencoder_config).to(self.device)
        self.autoencoder.load_state_dict(self.checkpoint["autoencoder_state_dict"])
        self.autoencoder.eval()
        
        # Diffusionç›¸å…³ï¼ˆä»…åœ¨diffusion checkpointä¸­åˆå§‹åŒ–ï¼‰
        if self.checkpoint_stage == "diffusion":
            # UNet
            self.unet = DiffusionModelUNet(**self.unet_config).to(self.device)
            self.unet.load_state_dict(self.checkpoint["unet_state_dict"])
            self.unet.eval()
            
            # Scheduler
            self.scheduler = DDPMScheduler(**self.scheduler_config)
            
            # Inferer
            self.inferer = LatentDiffusionInferer(self.scheduler, scale_factor=self.scale_factor)
        else:
            self.unet = None
            self.scheduler = None
            self.inferer = None
            print("âš ï¸  è¿™æ˜¯AutoEncoder checkpointï¼Œåªèƒ½æµ‹è¯•AutoEncoderåŠŸèƒ½")
    
    def load_test_data(self, num_images=10):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
        print("="*60)
        
        # è¯»å–TIFF
        images = tifffile.imread(self.tiff_path)
        if images.ndim == 2:
            images = images[np.newaxis, ...]
        
        # åªå–å‰num_imageså¼ 
        images = images[:num_images]
        
        # ç¡®å®šå½’ä¸€åŒ–èŒƒå›´
        if images.dtype == np.uint8:
            scale_max = 255.0
        elif images.dtype == np.uint16:
            scale_max = 65535.0
        else:
            scale_max = float(images.max())
        
        print(f"âœ… åŠ è½½ {len(images)} å¼ å›¾åƒ")
        print(f"   - æ•°æ®èŒƒå›´: [0, {scale_max}]")
        
        # åˆ›å»ºæ•°æ®é›†
        val_transforms = transforms.Compose([
            transforms.ScaleIntensityRanged(
                keys=["image"], a_min=0.0, a_max=scale_max,
                b_min=0.0, b_max=1.0, clip=True
            ),
            transforms.Resized(
                keys=["image"], 
                spatial_size=[self.image_size, self.image_size]
            ),
        ])
        
        dataset = TiffStackDataset(
            images_array=images,
            transform=val_transforms
        )
        
        loader = DataLoader(dataset, batch_size=1, shuffle=False)
        
        return loader
    
    def test_1_autoencoder_quality(self, data_loader):
        """æµ‹è¯•1: AutoEncoderé‡å»ºè´¨é‡"""
        print("\n" + "="*60)
        print("ğŸ” æµ‹è¯•1: AutoEncoderé‡å»ºè´¨é‡")
        print("="*60)
        
        recon_losses = []
        latent_stats = []
        
        with torch.no_grad():
            for i, batch in enumerate(data_loader):
                if i >= 5:  # åªæµ‹è¯•å‰5å¼ 
                    break
                    
                images = batch["image"].to(self.device)
                
                with autocast(enabled=True):
                    # é‡å»º
                    reconstruction, z_mu, z_sigma = self.autoencoder(images)
                    
                    # è®¡ç®—æŸå¤±
                    recon_loss = F.l1_loss(reconstruction.float(), images.float())
                    recon_losses.append(recon_loss.item())
                    
                    # æ½œåœ¨ç©ºé—´ç»Ÿè®¡
                    z = self.autoencoder.sampling(z_mu, z_sigma)
                    latent_stats.append({
                        "mean": z.mean().item(),
                        "std": z.std().item(),
                        "min": z.min().item(),
                        "max": z.max().item(),
                    })
        
        avg_recon_loss = np.mean(recon_losses)
        
        print(f"\nğŸ“Š é‡å»ºæŸå¤± (L1): {avg_recon_loss:.6f}")
        print(f"   {'âœ… ä¼˜ç§€' if avg_recon_loss < 0.05 else 'âš ï¸ è¾ƒå·®' if avg_recon_loss < 0.1 else 'âŒ å¾ˆå·®'}")
        
        print(f"\nğŸ“Š æ½œåœ¨ç©ºé—´ç»Ÿè®¡:")
        avg_stats = {k: np.mean([s[k] for s in latent_stats]) for k in latent_stats[0].keys()}
        for k, v in avg_stats.items():
            print(f"   {k}: {v:.4f}")
        
        # åˆ¤æ–­æ½œåœ¨ç©ºé—´æ˜¯å¦æ­£å¸¸
        if abs(avg_stats["mean"]) > 0.5:
            print(f"   âš ï¸ è­¦å‘Š: æ½œåœ¨ç©ºé—´å‡å€¼åç¦»0è¾ƒå¤š ({avg_stats['mean']:.4f})")
        if avg_stats["std"] < 0.5 or avg_stats["std"] > 2.0:
            print(f"   âš ï¸ è­¦å‘Š: æ½œåœ¨ç©ºé—´æ ‡å‡†å·®å¼‚å¸¸ ({avg_stats['std']:.4f}ï¼ŒæœŸæœ›æ¥è¿‘1.0)")
        
        # å¯è§†åŒ–é‡å»º
        self._visualize_reconstruction(data_loader)
        
        return avg_recon_loss
    
    def test_2_scaling_factor(self, data_loader):
        """æµ‹è¯•2: Scaling Factoråˆç†æ€§"""
        print("\n" + "="*60)
        print("ğŸ” æµ‹è¯•2: Scaling Factor")
        print("="*60)
        
        # è®¡ç®—scaling factor
        with torch.no_grad():
            batch = next(iter(data_loader))
            images = batch["image"].to(self.device)
            with autocast(enabled=True):
                z = self.autoencoder.encode_stage_2_inputs(images)
        
        computed_scale = 1 / torch.std(z)
        print(f"ğŸ“Š è®¡ç®—çš„Scale Factor: {computed_scale.item():.4f}")
        
        if self.scale_factor is not None:
            print(f"ğŸ“Š ä¿å­˜çš„Scale Factor: {self.scale_factor:.4f}")
            diff = abs(self.scale_factor - computed_scale.item())
            if diff > 0.1:
                print(f"   âš ï¸ è­¦å‘Š: Scale factorå·®å¼‚è¾ƒå¤§ (å·®å€¼: {diff:.4f})")
            else:
                print(f"   âœ… Scale factoræ­£å¸¸")
        else:
            print(f"   ğŸ’¡ å»ºè®®ä½¿ç”¨æ­¤Scale Factorè®­ç»ƒDiffusionæ¨¡å‹")
        
        return computed_scale.item()
    
    def test_3_diffusion_forward(self, data_loader):
        """æµ‹è¯•3: Diffusionå‰å‘ä¼ æ’­"""
        if self.checkpoint_stage != "diffusion":
            print("\nâ­ï¸  è·³è¿‡æµ‹è¯•3: éœ€è¦Diffusion checkpoint")
            return
        
        print("\n" + "="*60)
        print("ğŸ” æµ‹è¯•3: Diffusionå‰å‘ä¼ æ’­")
        print("="*60)
        
        losses_by_timestep = {t: [] for t in [0, 250, 500, 750, 999]}
        
        with torch.no_grad():
            for i, batch in enumerate(data_loader):
                if i >= 3:
                    break
                
                images = batch["image"].to(self.device)
                
                with autocast(enabled=True):
                    # ç¼–ç 
                    z_mu, z_sigma = self.autoencoder.encode(images)
                    z = self.autoencoder.sampling(z_mu, z_sigma)
                    
                    # æµ‹è¯•ä¸åŒtimestepçš„é¢„æµ‹
                    for t in losses_by_timestep.keys():
                        noise = torch.randn_like(z)
                        timesteps = torch.tensor([t], device=self.device).long()
                        
                        noise_pred = self.inferer(
                            inputs=images,
                            diffusion_model=self.unet,
                            noise=noise,
                            timesteps=timesteps,
                            autoencoder_model=self.autoencoder
                        )
                        
                        loss = F.mse_loss(noise_pred.float(), noise.float())
                        losses_by_timestep[t].append(loss.item())
        
        print(f"\nğŸ“Š ä¸åŒtimestepçš„é¢„æµ‹æŸå¤±:")
        for t, losses in losses_by_timestep.items():
            avg_loss = np.mean(losses)
            print(f"   t={t:4d}: {avg_loss:.6f}")
        
        # æ£€æŸ¥æ˜¯å¦å­¦åˆ°äº†ä¸œè¥¿
        early_loss = np.mean(losses_by_timestep[0])
        late_loss = np.mean(losses_by_timestep[999])
        
        if early_loss > late_loss * 1.5:
            print(f"\n   âŒ å¼‚å¸¸: æ—©æœŸtimestepæŸå¤±è¿‡é«˜ï¼Œæ¨¡å‹å¯èƒ½æ²¡å­¦å¥½")
        elif abs(early_loss - late_loss) < 0.01:
            print(f"\n   âš ï¸ è­¦å‘Š: å„timestepæŸå¤±å‡ ä¹ç›¸åŒï¼Œæ¨¡å‹å¯èƒ½æ¬ æ‹Ÿåˆ")
        else:
            print(f"\n   âœ… TimestepæŸå¤±åˆ†å¸ƒæ­£å¸¸")
    
    def test_4_sampling_process(self, num_samples=4, num_steps=1000):
        """æµ‹è¯•4: é‡‡æ ·è¿‡ç¨‹"""
        if self.checkpoint_stage != "diffusion":
            print("\nâ­ï¸  è·³è¿‡æµ‹è¯•4: éœ€è¦Diffusion checkpoint")
            return None
        
        print("\n" + "="*60)
        print(f"ğŸ” æµ‹è¯•4: é‡‡æ ·è¿‡ç¨‹ ({num_steps}æ­¥)")
        print("="*60)
        
        self.scheduler.set_timesteps(num_inference_steps=num_steps)
        
        samples = []
        intermediates_list = []
        
        with torch.no_grad():
            for i in range(num_samples):
                print(f"   ç”Ÿæˆæ ·æœ¬ {i+1}/{num_samples}...", end=" ")
                
                # åˆå§‹å™ªå£°
                noise = torch.randn(
                    (1, self.autoencoder_config["latent_channels"],
                     self.latent_size, self.latent_size)
                ).to(self.device)
                
                # é‡‡æ ·
                with autocast(enabled=True):
                    sample, intermediates = self.inferer.sample(
                        input_noise=noise,
                        diffusion_model=self.unet,
                        scheduler=self.scheduler,
                        autoencoder_model=self.autoencoder,
                        save_intermediates=True,
                        intermediate_steps=num_steps // 5,  # ä¿å­˜5ä¸ªä¸­é—´æ­¥
                    )
                
                samples.append(sample[0, 0].cpu().numpy())
                intermediates_list.append([img[0, 0].cpu().numpy() for img in intermediates])
                print("âœ“")
        
        # ç»Ÿè®¡ç”Ÿæˆå›¾åƒ
        samples_array = np.stack(samples)
        print(f"\nğŸ“Š ç”Ÿæˆå›¾åƒç»Ÿè®¡:")
        print(f"   å‡å€¼: {samples_array.mean():.4f}")
        print(f"   æ ‡å‡†å·®: {samples_array.std():.4f}")
        print(f"   èŒƒå›´: [{samples_array.min():.4f}, {samples_array.max():.4f}]")
        
        # æ£€æŸ¥å¼‚å¸¸
        if samples_array.std() < 0.05:
            print(f"   âŒ ä¸¥é‡é—®é¢˜: ç”Ÿæˆå›¾åƒå‡ ä¹æ˜¯å¸¸æ•°ï¼ˆæ ‡å‡†å·®è¿‡ä½ï¼‰")
        elif abs(samples_array.mean() - 0.5) > 0.3:
            print(f"   âš ï¸ è­¦å‘Š: ç”Ÿæˆå›¾åƒå‡å€¼åç¦»0.5è¾ƒå¤š")
        else:
            print(f"   âœ… ç»Ÿè®¡ç‰¹å¾æ­£å¸¸")
        
        # å¯è§†åŒ–
        self._visualize_samples(samples, intermediates_list)
        
        return samples
    
    def test_5_compare_with_real(self, data_loader, generated_samples):
        """æµ‹è¯•5: ä¸çœŸå®å›¾åƒå¯¹æ¯”"""
        if generated_samples is None:
            print("\nâ­ï¸  è·³è¿‡æµ‹è¯•5: éœ€è¦Diffusion checkpoint")
            return
        
        print("\n" + "="*60)
        print("ğŸ” æµ‹è¯•5: ç”Ÿæˆå›¾åƒ vs çœŸå®å›¾åƒ")
        print("="*60)
        
        # è·å–çœŸå®å›¾åƒç»Ÿè®¡
        real_stats = []
        with torch.no_grad():
            for i, batch in enumerate(data_loader):
                if i >= 5:
                    break
                images = batch["image"].cpu().numpy()
                real_stats.append({
                    "mean": images.mean(),
                    "std": images.std(),
                    "min": images.min(),
                    "max": images.max(),
                })
        
        real_avg = {k: np.mean([s[k] for s in real_stats]) for k in real_stats[0].keys()}
        gen_array = np.stack(generated_samples)
        
        print(f"\nğŸ“Š å¯¹æ¯”:")
        print(f"   {'æŒ‡æ ‡':<10} {'çœŸå®å›¾åƒ':<15} {'ç”Ÿæˆå›¾åƒ':<15} {'å·®å¼‚'}")
        print(f"   {'-'*50}")
        print(f"   {'å‡å€¼':<10} {real_avg['mean']:<15.4f} {gen_array.mean():<15.4f} {abs(real_avg['mean'] - gen_array.mean()):.4f}")
        print(f"   {'æ ‡å‡†å·®':<10} {real_avg['std']:<15.4f} {gen_array.std():<15.4f} {abs(real_avg['std'] - gen_array.std()):.4f}")
        print(f"   {'æœ€å°å€¼':<10} {real_avg['min']:<15.4f} {gen_array.min():<15.4f} {abs(real_avg['min'] - gen_array.min()):.4f}")
        print(f"   {'æœ€å¤§å€¼':<10} {real_avg['max']:<15.4f} {gen_array.max():<15.4f} {abs(real_avg['max'] - gen_array.max()):.4f}")
        
        # åˆ¤æ–­
        mean_diff = abs(real_avg['mean'] - gen_array.mean())
        std_diff = abs(real_avg['std'] - gen_array.std())
        
        if mean_diff > 0.2 or std_diff > 0.15:
            print(f"\n   âŒ ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒç»Ÿè®¡ç‰¹å¾å·®å¼‚è¾ƒå¤§")
        else:
            print(f"\n   âœ… ç”Ÿæˆå›¾åƒç»Ÿè®¡ç‰¹å¾æ¥è¿‘çœŸå®å›¾åƒ")
    
    def _visualize_reconstruction(self, data_loader):
        """å¯è§†åŒ–AutoEncoderé‡å»º"""
        with torch.no_grad():
            batch = next(iter(data_loader))
            images = batch["image"].to(self.device)
            
            with autocast(enabled=True):
                reconstruction, _, _ = self.autoencoder(images)
            
            orig = images[0, 0].cpu().numpy()
            recon = reconstruction[0, 0].cpu().numpy()
        
        fig, axes = plt.subplots(1, 3, figsize=(12, 4))
        
        axes[0].imshow(orig, cmap='gray')
        axes[0].set_title("åŸå§‹å›¾åƒ")
        axes[0].axis('off')
        
        axes[1].imshow(recon, cmap='gray')
        axes[1].set_title("é‡å»ºå›¾åƒ")
        axes[1].axis('off')
        
        diff = np.abs(orig - recon)
        im = axes[2].imshow(diff, cmap='hot')
        axes[2].set_title(f"å·®å¼‚å›¾ (MAE={diff.mean():.4f})")
        axes[2].axis('off')
        plt.colorbar(im, ax=axes[2])
        
        plt.tight_layout()
        output_path = Path(self.checkpoint_path).parent.parent / "debug_reconstruction.png"
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"\nğŸ’¾ é‡å»ºå¯¹æ¯”å·²ä¿å­˜: {output_path}")
    
    def _visualize_samples(self, samples, intermediates_list):
        """å¯è§†åŒ–ç”Ÿæˆæ ·æœ¬å’Œå»å™ªè¿‡ç¨‹"""
        num_samples = len(samples)
        
        # 1. æœ€ç»ˆç”Ÿæˆæ ·æœ¬
        fig, axes = plt.subplots(1, num_samples, figsize=(4*num_samples, 4))
        if num_samples == 1:
            axes = [axes]
        
        for i, sample in enumerate(samples):
            axes[i].imshow(sample, cmap='gray', vmin=0, vmax=1)
            axes[i].set_title(f"æ ·æœ¬ {i+1}")
            axes[i].axis('off')
        
        plt.tight_layout()
        output_path = Path(self.checkpoint_path).parent.parent / "debug_generated_samples.png"
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ’¾ ç”Ÿæˆæ ·æœ¬å·²ä¿å­˜: {output_path}")
        
        # 2. å»å™ªè¿‡ç¨‹
        fig, axes = plt.subplots(num_samples, len(intermediates_list[0]), 
                                figsize=(len(intermediates_list[0])*3, num_samples*3))
        if num_samples == 1:
            axes = axes[np.newaxis, :]
        
        for i in range(num_samples):
            for j, intermediate in enumerate(intermediates_list[i]):
                axes[i, j].imshow(intermediate, cmap='gray', vmin=0, vmax=1)
                axes[i, j].set_title(f"æ ·æœ¬{i+1} - æ­¥éª¤{j+1}")
                axes[i, j].axis('off')
        
        plt.tight_layout()
        output_path = Path(self.checkpoint_path).parent.parent / "debug_denoising_process.png"
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ’¾ å»å™ªè¿‡ç¨‹å·²ä¿å­˜: {output_path}")
    
    def run_full_diagnosis(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("\n" + "="*60)
        print("ğŸ”¬ å¼€å§‹å®Œæ•´è¯Šæ–­")
        print("="*60)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        data_loader = self.load_test_data(num_images=10)
        
        # æµ‹è¯•1: AutoEncoderè´¨é‡
        recon_loss = self.test_1_autoencoder_quality(data_loader)
        
        # æµ‹è¯•2: Scaling Factor
        scale_factor = self.test_2_scaling_factor(data_loader)
        
        # æµ‹è¯•3: Diffusionå‰å‘ä¼ æ’­
        self.test_3_diffusion_forward(data_loader)
        
        # æµ‹è¯•4: é‡‡æ ·è¿‡ç¨‹
        generated_samples = self.test_4_sampling_process(num_samples=4, num_steps=1000)
        
        # æµ‹è¯•5: ä¸çœŸå®å›¾åƒå¯¹æ¯”
        self.test_5_compare_with_real(data_loader, generated_samples)
        
        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“‹ è¯Šæ–­æ€»ç»“")
        print("="*60)
        
        issues = []
        suggestions = []
        
        # AutoEncoderç›¸å…³é—®é¢˜
        if recon_loss > 0.1:
            issues.append("âŒ AutoEncoderé‡å»ºè´¨é‡å·®")
            suggestions.append("   â†’ ç»§ç»­è®­ç»ƒAutoEncoderæˆ–é™ä½å­¦ä¹ ç‡")
        elif recon_loss > 0.05:
            issues.append("âš ï¸ AutoEncoderé‡å»ºè´¨é‡ä¸€èˆ¬")
            suggestions.append("   â†’ å»ºè®®ç»§ç»­è®­ç»ƒä»¥æå‡è´¨é‡")
        
        # Scaling Factoré—®é¢˜
        if self.scale_factor is not None and abs(self.scale_factor - scale_factor) > 0.1:
            issues.append("âš ï¸ Scaling Factorä¸å‡†ç¡®")
            suggestions.append("   â†’ é‡æ–°è®¡ç®—å¹¶æ›´æ–°scaling factor")
        
        # æ ¹æ®checkpointç±»å‹ç»™å‡ºå»ºè®®
        if self.checkpoint_stage == "autoencoder":
            print("\nğŸ“Œ å½“å‰çŠ¶æ€: AutoEncoderè®­ç»ƒå®Œæˆ")
            print(f"\n   AutoEncoderé‡å»ºLoss: {recon_loss:.6f}")
            print(f"   å»ºè®®Scaling Factor: {scale_factor:.4f}")
            
            if recon_loss < 0.05:
                print("\nâœ… AutoEncoderè´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒDiffusionæ¨¡å‹ï¼")
                print("\nä¸‹ä¸€æ­¥:")
                print("   python train_tiff_ldm.py \\")
                print("       --tiff_path ./data/mt.tif \\")
                print("       --skip_autoencoder \\")
                print(f"       --autoencoder_checkpoint {self.checkpoint_path}")
            else:
                print("\nâš ï¸ å»ºè®®ç»§ç»­è®­ç»ƒAutoEncoderä»¥æå‡è´¨é‡")
        
        elif self.checkpoint_stage == "diffusion":
            if len(issues) == 0:
                print("âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜ï¼ŒDiffusionæ¨¡å‹å¯èƒ½éœ€è¦ï¼š")
                print("   1. ç»§ç»­è®­ç»ƒæ›´å¤šepoch")
                print("   2. å¢åŠ è®­ç»ƒæ•°æ®é‡")
                print("   3. è°ƒæ•´å­¦ä¹ ç‡")
                print("   4. å°è¯•ä¸åŒçš„é‡‡æ ·æ­¥æ•° (å¦‚2000æ­¥)")
            else:
                print("å‘ç°ä»¥ä¸‹é—®é¢˜:")
                for issue in issues:
                    print(f"  {issue}")
                print("\nå»ºè®®:")
                for suggestion in suggestions:
                    print(f"  {suggestion}")
        
        print("="*60)


def main():
    parser = argparse.ArgumentParser(description="Diffusionæ¨¡å‹è¯Šæ–­å·¥å…·")
    parser.add_argument("--checkpoint", type=str, required=True, 
                       help="Diffusion checkpointè·¯å¾„")
    parser.add_argument("--tiff_path", type=str, required=True,
                       help="TIFFæ•°æ®è·¯å¾„")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--num_samples", type=int, default=4,
                       help="ç”Ÿæˆæ ·æœ¬æ•°é‡")
    parser.add_argument("--num_steps", type=int, default=1000,
                       help="é‡‡æ ·æ­¥æ•°")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶
    if not Path(args.checkpoint).exists():
        print(f"âŒ Checkpointä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    if not Path(args.tiff_path).exists():
        print(f"âŒ TIFFæ–‡ä»¶ä¸å­˜åœ¨: {args.tiff_path}")
        return
    
    # è¿è¡Œè¯Šæ–­
    debugger = DiffusionDebugger(args.checkpoint, args.tiff_path, args.device)
    debugger.run_full_diagnosis()


if __name__ == "__main__":
    main()

