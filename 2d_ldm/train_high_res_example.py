"""
é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆè®­ç»ƒç¤ºä¾‹
æ”¯æŒ512Ã—512å’Œ1024Ã—1024
åŒ…å«å¤šç§æ˜¾å­˜ä¼˜åŒ–æŠ€å·§
"""

import os
import torch
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm

# é€‰æ‹©é…ç½®
USE_1024 = False  # æ”¹ä¸ºTrueä½¿ç”¨1024Ã—1024é…ç½®

if USE_1024:
    from config_1024_optimized import *
    print("ğŸš€ ä½¿ç”¨1024Ã—1024é…ç½®")
else:
    from config_512 import *
    print("ğŸš€ ä½¿ç”¨512Ã—512é…ç½®")

# ============================================
# ç¤ºä¾‹ï¼šå¸¦æ¢¯åº¦ç´¯ç§¯çš„è®­ç»ƒå¾ªç¯
# ============================================

def train_autoencoder_with_optimization(
    autoencoderkl,
    discriminator,
    train_loader,
    val_loader,
    device,
    config
):
    """
    ä¼˜åŒ–ç‰ˆAutoencoderKLè®­ç»ƒå¾ªç¯
    åŒ…å«æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦è£å‰ªç­‰æŠ€å·§
    """
    
    # æŸå¤±å‡½æ•°
    perceptual_loss, adv_loss = get_losses(device)
    
    # ä¼˜åŒ–å™¨
    optimizer_g = torch.optim.Adam(
        autoencoderkl.parameters(), 
        lr=config['learning_rate_g']
    )
    optimizer_d = torch.optim.Adam(
        discriminator.parameters(), 
        lr=config['learning_rate_d']
    )
    
    # æ··åˆç²¾åº¦
    scaler_g = GradScaler()
    scaler_d = GradScaler()
    
    # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    accumulation_steps = OPTIMIZATION.get('gradient_accumulation_steps', 1) if USE_1024 else 1
    
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   - æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯: {accumulation_steps}")
    print(f"   - ç­‰æ•ˆæ‰¹æ¬¡: {BATCH_SIZE * accumulation_steps}")
    
    n_epochs = config['n_epochs']
    val_interval = config['val_interval']
    
    for epoch in range(n_epochs):
        autoencoderkl.train()
        discriminator.train()
        
        epoch_loss = 0
        gen_epoch_loss = 0
        disc_epoch_loss = 0
        
        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=120)
        progress_bar.set_description(f"Epoch {epoch}")
        
        for step, batch in progress_bar:
            images = batch["image"].to(device)
            
            # ===== Generatorè®­ç»ƒ =====
            with autocast(enabled=True):
                reconstruction, z_mu, z_sigma = autoencoderkl(images)
                
                # é‡å»ºæŸå¤±
                recons_loss = F.l1_loss(reconstruction.float(), images.float())
                
                # æ„ŸçŸ¥æŸå¤±
                p_loss = perceptual_loss(reconstruction.float(), images.float())
                
                # KLæ•£åº¦
                kl_loss = 0.5 * torch.sum(
                    z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, 
                    dim=[1, 2, 3]
                )
                kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]
                
                # æ€»æŸå¤±
                loss_g = recons_loss + \
                         (config['kl_weight'] * kl_loss) + \
                         (config['perceptual_weight'] * p_loss)
                
                # å¯¹æŠ—æŸå¤±ï¼ˆé¢„çƒ­åï¼‰
                if epoch > config['autoencoder_warm_up_n_epochs']:
                    logits_fake = discriminator(reconstruction.contiguous().float())[-1]
                    generator_loss = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)
                    loss_g += config['adv_weight'] * generator_loss
                
                # æ¢¯åº¦ç´¯ç§¯ï¼šé™¤ä»¥ç´¯ç§¯æ­¥æ•°
                loss_g = loss_g / accumulation_steps
            
            # åå‘ä¼ æ’­
            scaler_g.scale(loss_g).backward()
            
            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
            if (step + 1) % accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰
                if USE_1024 and OPTIMIZATION.get('max_grad_norm'):
                    scaler_g.unscale_(optimizer_g)
                    torch.nn.utils.clip_grad_norm_(
                        autoencoderkl.parameters(), 
                        OPTIMIZATION['max_grad_norm']
                    )
                
                scaler_g.step(optimizer_g)
                scaler_g.update()
                optimizer_g.zero_grad(set_to_none=True)
            
            # ===== Discriminatorè®­ç»ƒ =====
            if epoch > config['autoencoder_warm_up_n_epochs']:
                with autocast(enabled=True):
                    logits_fake = discriminator(reconstruction.contiguous().detach())[-1]
                    loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)
                    
                    logits_real = discriminator(images.contiguous().detach())[-1]
                    loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)
                    
                    discriminator_loss = (loss_d_fake + loss_d_real) * 0.5
                    loss_d = config['adv_weight'] * discriminator_loss / accumulation_steps
                
                scaler_d.scale(loss_d).backward()
                
                if (step + 1) % accumulation_steps == 0:
                    scaler_d.step(optimizer_d)
                    scaler_d.update()
                    optimizer_d.zero_grad(set_to_none=True)
                
                disc_epoch_loss += discriminator_loss.item()
            
            epoch_loss += recons_loss.item()
            if epoch > config['autoencoder_warm_up_n_epochs']:
                gen_epoch_loss += generator_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                "recons": f"{epoch_loss / (step + 1):.4f}",
                "gen": f"{gen_epoch_loss / (step + 1):.4f}",
                "disc": f"{disc_epoch_loss / (step + 1):.4f}",
                "mem": f"{torch.cuda.max_memory_allocated(device)/1024**3:.1f}GB"
            })
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜ï¼ˆå¯¹1024Ã—1024æœ‰å¸®åŠ©ï¼‰
            if USE_1024 and step % 50 == 0:
                torch.cuda.empty_cache()
        
        # éªŒè¯
        if (epoch + 1) % val_interval == 0:
            val_loss = validate(autoencoderkl, val_loader, device)
            print(f"Epoch {epoch + 1} - Val Loss: {val_loss:.4f}")
            
            # ä¿å­˜checkpoint
            save_checkpoint(autoencoderkl, discriminator, epoch, val_loss)
        
        progress_bar.close()
    
    return autoencoderkl, discriminator


def validate(model, val_loader, device):
    """éªŒè¯å‡½æ•°"""
    model.eval()
    val_loss = 0
    
    with torch.no_grad():
        for batch in val_loader:
            images = batch["image"].to(device)
            with autocast(enabled=True):
                reconstruction, _, _ = model(images)
                loss = F.l1_loss(reconstruction.float(), images.float())
                val_loss += loss.item()
    
    model.train()
    return val_loss / len(val_loader)


def save_checkpoint(model, discriminator, epoch, loss):
    """ä¿å­˜æ¨¡å‹checkpoint"""
    checkpoint_dir = "checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'discriminator_state_dict': discriminator.state_dict(),
        'loss': loss,
    }
    
    filename = f"{checkpoint_dir}/checkpoint_epoch_{epoch}_res_{IMAGE_SIZE}.pth"
    torch.save(checkpoint, filename)
    print(f"ğŸ’¾ Checkpointä¿å­˜: {filename}")


# ============================================
# æ¸è¿›å¼è®­ç»ƒå‡½æ•°ï¼ˆæ¨èç”¨äº1024Ã—1024ï¼‰
# ============================================

def progressive_training(train_data, val_data, device):
    """
    æ¸è¿›å¼è®­ç»ƒï¼šä»ä½åˆ†è¾¨ç‡å¼€å§‹ï¼Œé€æ­¥æå‡
    è¿™æ˜¯è®­ç»ƒ1024Ã—1024çš„æœ€ä½³ç­–ç•¥
    """
    
    if not USE_1024 or not PROGRESSIVE_TRAINING['enabled']:
        print("âš ï¸ æ¸è¿›å¼è®­ç»ƒæœªå¯ç”¨")
        return None
    
    print("ğŸ¯ å¼€å§‹æ¸è¿›å¼è®­ç»ƒ...")
    
    stages = PROGRESSIVE_TRAINING['stages']
    model = None
    discriminator = None
    
    for stage_idx, stage in enumerate(stages):
        resolution = stage['resolution']
        epochs = stage['epochs']
        batch_size = stage['batch_size']
        
        print(f"\n{'='*50}")
        print(f"é˜¶æ®µ {stage_idx + 1}: {resolution}Ã—{resolution}")
        print(f"Epochs: {epochs}, Batch Size: {batch_size}")
        print(f"{'='*50}\n")
        
        # å‡†å¤‡è¯¥åˆ†è¾¨ç‡çš„æ•°æ®åŠ è½½å™¨
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ•°æ®åŠ è½½ä»£ç 
        # train_loader = prepare_dataloader(train_data, resolution, batch_size)
        # val_loader = prepare_dataloader(val_data, resolution, batch_size)
        
        # åˆå§‹åŒ–æˆ–æ›´æ–°æ¨¡å‹
        if model is None:
            # ç¬¬ä¸€æ¬¡åˆ›å»ºæ¨¡å‹
            model = get_autoencoderkl(device)
            discriminator = get_discriminator(device)
        else:
            # ä»ä¸Šä¸€é˜¶æ®µç»§ç»­ï¼ˆå¯èƒ½éœ€è¦è°ƒæ•´æŸäº›å±‚ï¼‰
            print("ğŸ“¦ ä»ä¸Šä¸€é˜¶æ®µåŠ è½½æ¨¡å‹...")
        
        # è®­ç»ƒè¯¥é˜¶æ®µ
        # model, discriminator = train_autoencoder_with_optimization(...)
    
    print("\nâœ… æ¸è¿›å¼è®­ç»ƒå®Œæˆï¼")
    return model, discriminator


# ============================================
# ä¸»å‡½æ•°ç¤ºä¾‹
# ============================================

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    print(f"ğŸ¨ å›¾åƒåˆ†è¾¨ç‡: {IMAGE_SIZE}Ã—{IMAGE_SIZE}")
    print(f"ğŸ’¾ å¯ç”¨æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # åˆå§‹åŒ–æ¨¡å‹
    autoencoderkl = get_autoencoderkl(device)
    discriminator = get_discriminator(device)
    
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°é‡:")
    print(f"   AutoencoderKL: {sum(p.numel() for p in autoencoderkl.parameters())/1e6:.2f}M")
    print(f"   Discriminator: {sum(p.numel() for p in discriminator.parameters())/1e6:.2f}M")
    
    # è¿™é‡Œéœ€è¦å‡†å¤‡å®é™…çš„æ•°æ®åŠ è½½å™¨
    # train_loader = ...
    # val_loader = ...
    
    print(f"\n{'='*60}")
    print("âš ï¸  è¿™æ˜¯ä¸€ä¸ªé…ç½®å’Œè®­ç»ƒæ¡†æ¶ç¤ºä¾‹")
    print("   å®é™…ä½¿ç”¨æ—¶éœ€è¦:")
    print("   1. å‡†å¤‡å¯¹åº”åˆ†è¾¨ç‡çš„æ•°æ®é›†")
    print("   2. è°ƒæ•´data transformsä¸­çš„spatial_size")
    print("   3. æ ¹æ®å®é™…æ˜¾å­˜æƒ…å†µå¾®è°ƒbatch_size")
    print("   4. ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜ä½¿ç”¨")
    print(f"{'='*60}\n")
    
    # è®­ç»ƒ
    # model, disc = train_autoencoder_with_optimization(
    #     autoencoderkl, discriminator, train_loader, val_loader,
    #     device, AUTOENCODER_CONFIG
    # )


if __name__ == "__main__":
    main()

