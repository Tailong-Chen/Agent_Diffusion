"""
åŸºäºMONAIçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒè„šæœ¬
ä¸“é—¨ç”¨äºå¤„ç†TIFFå †æ ˆæ•°æ® (1024Ã—1024å›¾åƒ)
é€‚é…32GBæ˜¾å­˜ç¯å¢ƒ

ä½¿ç”¨æ–¹æ³•:
    python train_tiff_ldm.py --tiff_path your_data.tif --output_dir ./output

ä½œè€…: åŸºäºMONAI GenerativeModelsä¿®æ”¹
"""

import os
import argparse
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
# PyTorch AMPå…¼å®¹æ€§å¤„ç†
import warnings
warnings.filterwarnings('ignore', message='.*torch.cuda.amp.autocast.*deprecated.*')
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import matplotlib.pyplot as plt
from pathlib import Path

# å›¾åƒè¯»å–åº“
try:
    import tifffile
    print("âœ… ä½¿ç”¨ tifffile è¯»å–TIFF")
except ImportError:
    print("âš ï¸  æœªå®‰è£… tifffileï¼Œå°è¯•ä½¿ç”¨ PIL")
    from PIL import Image
    tifffile = None

# MONAI å’Œ Generative Models
from monai import transforms
from monai.utils import set_determinism

from generative.inferers import LatentDiffusionInferer
from generative.losses.adversarial_loss import PatchAdversarialLoss
from generative.losses.perceptual import PerceptualLoss
from generative.networks.nets import AutoencoderKL, DiffusionModelUNet, PatchDiscriminator
from generative.networks.schedulers import DDPMScheduler


# ============================================
# 1. TIFFå †æ ˆæ•°æ®é›†ç±»
# ============================================

class TiffStackDataset(Dataset):
    """
    ç”¨äºåŠ è½½TIFFå †æ ˆçš„æ•°æ®é›†ç±»
    æ”¯æŒå•ä¸ªTIFFæ–‡ä»¶åŒ…å«å¤šå¼ å›¾åƒ
    """
    
    def __init__(self, tiff_path=None, transform=None, max_images=None, images_array=None):
        """
        Args:
            tiff_path: TIFFæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœimages_arrayä¸ºNoneåˆ™ä»æ–‡ä»¶åŠ è½½ï¼‰
            transform: MONAI transforms
            max_images: æœ€å¤šåŠ è½½å¤šå°‘å¼ å›¾åƒï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            images_array: ç›´æ¥æä¾›çš„å›¾åƒæ•°ç»„ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        """
        self.tiff_path = tiff_path
        self.transform = transform
        
        if images_array is not None:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„å›¾åƒæ•°ç»„
            self.images = images_array
        else:
            # ä»æ–‡ä»¶åŠ è½½
            print(f"ğŸ“‚ åŠ è½½TIFFæ–‡ä»¶: {tiff_path}")
            
            # è¯»å–TIFFå †æ ˆ
            if tifffile is not None:
                # ä½¿ç”¨tifffileåº“ï¼ˆæ¨èï¼‰
                self.images = tifffile.imread(tiff_path)
            else:
                # ä½¿ç”¨PILï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
                self.images = self._load_with_pil(tiff_path)
            
            # ç¡®ä¿æ˜¯3Dæ•°ç»„ (N, H, W)
            if self.images.ndim == 2:
                self.images = self.images[np.newaxis, ...]  # å•å¼ å›¾åƒ
            elif self.images.ndim == 4:
                # å¦‚æœæ˜¯ (N, H, W, C)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
                if self.images.shape[-1] in [1, 3, 4]:
                    self.images = self.images[..., 0]
                else:
                    self.images = self.images[:, :, :, 0]
            
            # é™åˆ¶å›¾åƒæ•°é‡
            if max_images is not None:
                self.images = self.images[:max_images]
            
            print(f"âœ… æˆåŠŸåŠ è½½ {self.images.shape[0]} å¼ å›¾åƒ")
            print(f"   å›¾åƒå½¢çŠ¶: {self.images.shape}")
            print(f"   æ•°æ®ç±»å‹: {self.images.dtype}")
            print(f"   æ•°å€¼èŒƒå›´: [{self.images.min():.2f}, {self.images.max():.2f}]")
        
        self.num_images = self.images.shape[0]
    
    def _load_with_pil(self, tiff_path):
        """ä½¿ç”¨PILåŠ è½½å¤šé¡µTIFF"""
        images = []
        img = Image.open(tiff_path)
        
        try:
            for i in range(1000):  # æœ€å¤šå°è¯•1000é¡µ
                img.seek(i)
                images.append(np.array(img))
        except EOFError:
            pass
        
        return np.stack(images)
    
    def __len__(self):
        return self.num_images
    
    def __getitem__(self, idx):
        # è·å–å›¾åƒ
        image = self.images[idx].astype(np.float32)
        
        # æ·»åŠ é€šé“ç»´åº¦: (H, W) -> (1, H, W)
        if image.ndim == 2:
            image = image[np.newaxis, ...]
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆMONAIé£æ ¼ï¼‰
        data_dict = {"image": image}
        
        # åº”ç”¨transforms
        if self.transform:
            data_dict = self.transform(data_dict)
        
        return data_dict


# ============================================
# 2. é…ç½®ç±»
# ============================================

class LDMConfig:
    """LDMè®­ç»ƒé…ç½®"""
    
    def __init__(self, image_size=1024, use_progressive=False):
        self.image_size = image_size
        self.use_progressive = use_progressive
        
        # æ•°æ®é…ç½®
        self.batch_size = 1 if image_size >= 1024 else 6  # 1024ç”¨batch=1ä»¥é€‚åº”æ›´å¤§æ¨¡å‹
        self.num_workers = 4
        self.train_split = 0.85  # 85%è®­ç»ƒï¼Œ15%éªŒè¯
        
        # AutoencoderKLé…ç½®
        # 1024 -> 512 -> 256 -> 128 (3å±‚ä¸‹é‡‡æ ·ï¼Œä¸‹é‡‡æ ·ç‡=8)
        # è½»é‡çº§é…ç½®ï¼šå‡å°‘é€šé“æ•°å’Œå‚æ•°é‡ä»¥èŠ‚çœæ˜¾å­˜ï¼Œç”¨é•¿æ—¶é—´è®­ç»ƒå¼¥è¡¥
        self.autoencoder_config = {
            "spatial_dims": 2,
            "in_channels": 1,
            "out_channels": 1,
            "num_channels": (64, 128, 256) if image_size >= 1024 else (128, 256, 512),  # è½»é‡çº§
            "latent_channels": 3 if image_size >= 1024 else 4,  # è½»é‡çº§
            "num_res_blocks": 1,  # è½»é‡çº§
            "attention_levels": (False, False, False) if image_size >= 1024 else (False, False, True),  # 1024æ— attention
            "with_encoder_nonlocal_attn": False,
            "with_decoder_nonlocal_attn": False,
        }
        
        # è®¡ç®—æ½œåœ¨ç©ºé—´å°ºå¯¸: image_size / (2^num_layers)
        num_downsample_layers = len(self.autoencoder_config["num_channels"])
        self.latent_size = image_size // (2 ** num_downsample_layers)
        
        # Discriminatoré…ç½®ï¼ˆè½»é‡çº§ï¼‰
        self.discriminator_config = {
            "spatial_dims": 2,
            "num_layers_d": 3,
            "num_channels": 32 if image_size >= 1024 else 64,  # 1024ç”¨æ›´å°çš„
            "in_channels": 1,
            "out_channels": 1,
        }
        
        # AutoencoderKLè®­ç»ƒå‚æ•°
        self.autoencoder_train = {
            "n_epochs": 10000 if image_size >= 1024 else 150,  # 1024ç”¨10000è½®é•¿æ—¶é—´è®­ç»ƒ
            "val_interval": 10,  # éªŒè¯é—´éš”ï¼ˆå·²ç”±ç”¨æˆ·è®¾ç½®ï¼‰
            "warm_up_epochs": 20,  # é¢„çƒ­æœŸï¼ˆé€‚ä¸­ï¼‰
            "lr_g": 5e-5,
            "lr_d": 2e-4,
            "kl_weight": 1e-5 if image_size >= 1024 else 1e-6,  # å¢å¼ºKLæ­£åˆ™ï¼Œä½¿æ½œåœ¨ç©ºé—´æ›´è§„æ•´
            "perceptual_weight": 0.001,
            "adv_weight": 0.01,
        }
        
        # DiffusionModelé…ç½®ï¼ˆè½»é‡çº§ï¼‰
        latent_ch = self.autoencoder_config["latent_channels"]
        if image_size >= 1024:
            # 1024Ã—1024å¢å¼ºé…ç½®ï¼ˆå¯ç”¨åä¸¤å±‚attentionæå‡è´¨é‡ï¼‰
            self.unet_config = {
                "spatial_dims": 2,
                "in_channels": latent_ch,
                "out_channels": latent_ch,
                "num_res_blocks": 2,  # å¢åŠ åˆ°2ï¼ˆæ›´å¼ºå­¦ä¹ èƒ½åŠ›ï¼‰
                "num_channels": (128, 256, 512, 768),  # å¢åŠ å±‚æ•°å’Œé€šé“
                "attention_levels": (False, False, True, True),  # âœ… å¯ç”¨åä¸¤å±‚attention
                "num_head_channels": (0, 0, 512, 768),  # âœ… åŒ¹é…attentionå±‚
            }
        else:
            # 512Ã—512æ ‡å‡†é…ç½®
            self.unet_config = {
                "spatial_dims": 2,
                "in_channels": latent_ch,
                "out_channels": latent_ch,
                "num_res_blocks": 2,
                "num_channels": (128, 256, 512, 768),
                "attention_levels": (False, False, True, True),
                "num_head_channels": (0, 0, 512, 768),
            }
        
        # Diffusionè®­ç»ƒå‚æ•°
        self.diffusion_train = {
            "n_epochs": 10000 if image_size >= 1024 else 250,  # 1024ä¹Ÿç”¨10000è½®
            "val_interval": 10,  # éªŒè¯é—´éš”å¢åŠ 
            "lr": 1e-6 if image_size >= 1024 else 1e-4,  # âœ… é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
            "warmup_steps": 500,  # âœ… å¢åŠ warmupæ­¥æ•°
        }
        
        # ä¼˜åŒ–é…ç½®ï¼ˆé’ˆå¯¹32GBæ˜¾å­˜ï¼‰
        # batch=8æ—¶çº¦20-24GBæ˜¾å­˜ï¼Œè¿˜æœ‰å……è¶³ä½™é‡
        self.optimization = {
            "use_amp": True,  # æ··åˆç²¾åº¦
            "gradient_accumulation_steps": 2 if image_size >= 1024 else 2,  # å‡å°‘ç´¯ç§¯ï¼Œé…åˆæ›´å¤§batch
            "max_grad_norm": 1.0,  # æ¢¯åº¦è£å‰ª
        }
        
        # Scheduleré…ç½®
        self.scheduler_config = {
            "num_train_timesteps": 1000,
            "schedule": "scaled_linear_beta",
            "beta_start": 0.00085,
            "beta_end": 0.012,
        }
    
    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“‹ LDMè®­ç»ƒé…ç½®")
        print("="*60)
        print(f"å›¾åƒåˆ†è¾¨ç‡: {self.image_size}Ã—{self.image_size}")
        print(f"æ½œåœ¨ç©ºé—´å°ºå¯¸: {self.latent_size}Ã—{self.latent_size}Ã—{self.autoencoder_config['latent_channels']}")
        print(f"ä¸‹é‡‡æ ·ç‡: {self.image_size // self.latent_size}x")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {self.optimization['gradient_accumulation_steps']}æ­¥")
        print(f"ç­‰æ•ˆæ‰¹æ¬¡å¤§å°: {self.batch_size * self.optimization['gradient_accumulation_steps']}")
        print(f"\nAutoEncoderè®­ç»ƒ:")
        print(f"  - Epochs: {self.autoencoder_train['n_epochs']}")
        print(f"  - é¢„çƒ­æœŸ: {self.autoencoder_train['warm_up_epochs']}")
        print(f"\nDiffusionè®­ç»ƒ:")
        print(f"  - Epochs: {self.diffusion_train['n_epochs']}")
        print(f"  - æ¨ç†æ­¥æ•°: {self.scheduler_config['num_train_timesteps']}")
        print("="*60 + "\n")


# ============================================
# 3. è®­ç»ƒå™¨ç±»
# ============================================

class LDMTrainer:
    """LDMè®­ç»ƒå™¨"""
    
    def __init__(self, config, output_dir, device):
        self.config = config
        self.output_dir = Path(output_dir)
        self.device = device
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "checkpoints").mkdir(exist_ok=True)
        (self.output_dir / "samples").mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # è®­ç»ƒå†å²
        self.history = {
            "autoencoder_train_loss": [],
            "autoencoder_val_loss": [],
            "diffusion_train_loss": [],
            "diffusion_val_loss": [],
        }
    
    def _init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
        
        # AutoencoderKL
        self.autoencoder = AutoencoderKL(**self.config.autoencoder_config).to(self.device)
        
        # Discriminator
        self.discriminator = PatchDiscriminator(**self.config.discriminator_config).to(self.device)
        
        # UNet (ç¨åè®­ç»ƒæ—¶åˆå§‹åŒ–)
        self.unet = None
        
        # æŸå¤±å‡½æ•°
        self.perceptual_loss = PerceptualLoss(spatial_dims=2, network_type="alex").to(self.device)
        self.perceptual_loss.eval()
        self.adv_loss = PatchAdversarialLoss(criterion="least_squares")
        
        # Scheduler
        self.scheduler = DDPMScheduler(**self.config.scheduler_config)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        ae_params = sum(p.numel() for p in self.autoencoder.parameters()) / 1e6
        disc_params = sum(p.numel() for p in self.discriminator.parameters()) / 1e6
        print(f"âœ… AutoencoderKLå‚æ•°é‡: {ae_params:.2f}M")
        print(f"âœ… Discriminatorå‚æ•°é‡: {disc_params:.2f}M")
    
    def train_autoencoder(self, train_loader, val_loader):
        """è®­ç»ƒAutoencoderKL"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ AutoencoderKL")
        print("="*60)
        
        cfg = self.config.autoencoder_train
        
        # ä¼˜åŒ–å™¨
        optimizer_g = torch.optim.Adam(self.autoencoder.parameters(), lr=cfg["lr_g"])
        optimizer_d = torch.optim.Adam(self.discriminator.parameters(), lr=cfg["lr_d"])
        
        # æ··åˆç²¾åº¦
        scaler_g = GradScaler()
        scaler_d = GradScaler()
        
        accumulation_steps = self.config.optimization["gradient_accumulation_steps"]
        
        for epoch in range(cfg["n_epochs"]):
            self.autoencoder.train()
            self.discriminator.train()
            
            epoch_loss = 0
            gen_loss_sum = 0
            disc_loss_sum = 0
            
            progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=120)
            progress_bar.set_description(f"Epoch {epoch+1}/{cfg['n_epochs']}")
            
            for step, batch in progress_bar:
                images = batch["image"].to(self.device)
                
                # ===== Generatorè®­ç»ƒ =====
                with autocast(enabled=self.config.optimization["use_amp"]):
                    reconstruction, z_mu, z_sigma = self.autoencoder(images)
                    
                    # é‡å»ºæŸå¤±
                    recons_loss = F.l1_loss(reconstruction.float(), images.float())
                    
                    # æ„ŸçŸ¥æŸå¤±
                    p_loss = self.perceptual_loss(reconstruction.float(), images.float())
                    
                    # KLæ•£åº¦
                    kl_loss = 0.5 * torch.sum(
                        z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1,
                        dim=[1, 2, 3]
                    )
                    kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]
                    
                    # æ€»æŸå¤±
                    loss_g = recons_loss + \
                             (cfg["kl_weight"] * kl_loss) + \
                             (cfg["perceptual_weight"] * p_loss)
                    
                    # å¯¹æŠ—æŸå¤±ï¼ˆé¢„çƒ­åï¼‰
                    generator_loss_val = 0
                    if epoch >= cfg["warm_up_epochs"]:
                        logits_fake = self.discriminator(reconstruction.contiguous().float())[-1]
                        generator_loss = self.adv_loss(logits_fake, target_is_real=True, for_discriminator=False)
                        loss_g += cfg["adv_weight"] * generator_loss
                        generator_loss_val = generator_loss.item()
                    
                    # æ¢¯åº¦ç´¯ç§¯
                    loss_g = loss_g / accumulation_steps
                
                scaler_g.scale(loss_g).backward()
                
                # æ›´æ–°Generator
                if (step + 1) % accumulation_steps == 0:
                    if self.config.optimization["max_grad_norm"]:
                        scaler_g.unscale_(optimizer_g)
                        torch.nn.utils.clip_grad_norm_(
                            self.autoencoder.parameters(),
                            self.config.optimization["max_grad_norm"]
                        )
                    scaler_g.step(optimizer_g)
                    scaler_g.update()
                    optimizer_g.zero_grad(set_to_none=True)
                
                # ===== Discriminatorè®­ç»ƒ =====
                discriminator_loss_val = 0
                if epoch >= cfg["warm_up_epochs"]:
                    with autocast(enabled=self.config.optimization["use_amp"]):
                        logits_fake = self.discriminator(reconstruction.contiguous().detach())[-1]
                        loss_d_fake = self.adv_loss(logits_fake, target_is_real=False, for_discriminator=True)
                        
                        logits_real = self.discriminator(images.contiguous().detach())[-1]
                        loss_d_real = self.adv_loss(logits_real, target_is_real=True, for_discriminator=True)
                        
                        discriminator_loss = (loss_d_fake + loss_d_real) * 0.5
                        loss_d = cfg["adv_weight"] * discriminator_loss / accumulation_steps
                        discriminator_loss_val = discriminator_loss.item()
                    
                    scaler_d.scale(loss_d).backward()
                    
                    if (step + 1) % accumulation_steps == 0:
                        scaler_d.step(optimizer_d)
                        scaler_d.update()
                        optimizer_d.zero_grad(set_to_none=True)
                
                epoch_loss += recons_loss.item()
                gen_loss_sum += generator_loss_val
                disc_loss_sum += discriminator_loss_val
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    "recons": f"{epoch_loss/(step+1):.4f}",
                    "gen": f"{gen_loss_sum/(step+1):.4f}",
                    "disc": f"{disc_loss_sum/(step+1):.4f}",
                    "mem": f"{torch.cuda.max_memory_allocated(self.device)/1024**3:.1f}GB"
                })
                
                # å®šæœŸæ¸…ç†ç¼“å­˜
                if step % 50 == 0:
                    torch.cuda.empty_cache()
            
            avg_train_loss = epoch_loss / len(train_loader)
            self.history["autoencoder_train_loss"].append(avg_train_loss)
            
            # éªŒè¯
            if (epoch + 1) % cfg["val_interval"] == 0:
                val_loss = self._validate_autoencoder(val_loader)
                self.history["autoencoder_val_loss"].append(val_loss)
                print(f"\nğŸ“Š Epoch {epoch+1} - Val Loss: {val_loss:.4f}")
                
                # ä¿å­˜checkpoint
                self._save_checkpoint("autoencoder", epoch, val_loss)
                
                # ä¿å­˜é‡å»ºæ ·æœ¬
                self._save_reconstruction_samples(images, reconstruction, epoch, "autoencoder")
            
            progress_bar.close()
        
        print("\nâœ… AutoencoderKLè®­ç»ƒå®Œæˆï¼")
        
        # è®¡ç®—scaling factorå¹¶ä¿å­˜æœ€ç»ˆcheckpoint
        print("ğŸ“ è®¡ç®—scaling factor...")
        scale_factor = self._compute_scale_factor(train_loader)
        print(f"âœ… Scaling factor: {scale_factor:.4f}")
        
        # ä¿å­˜åŒ…å«scale_factorçš„æœ€ç»ˆcheckpoint
        final_checkpoint = {
            "epoch": cfg["n_epochs"] - 1,
            "stage": "autoencoder",
            "loss": avg_train_loss,
            "config": self.config.__dict__,
            "autoencoder_state_dict": self.autoencoder.state_dict(),
            "scale_factor": scale_factor,
        }
        final_filename = self.output_dir / "checkpoints" / f"autoencoder_final.pth"
        torch.save(final_checkpoint, final_filename)
        print(f"ğŸ’¾ æœ€ç»ˆCheckpointå·²ä¿å­˜: {final_filename}")
        
        # æ¸…ç†
        del self.discriminator
        del self.perceptual_loss
        torch.cuda.empty_cache()
        
        return scale_factor
    
    def train_diffusion(self, train_loader, val_loader, scale_factor=None):
        """è®­ç»ƒDiffusionæ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ Diffusion Model")
        print("="*60)
        
        # è®¡ç®—scaling factor
        if scale_factor is None:
            print("ğŸ“ è®¡ç®—scaling factor...")
            scale_factor = self._compute_scale_factor(train_loader)
        print(f"âœ… Scaling factor: {scale_factor:.4f}")
        
        # åˆå§‹åŒ–UNet
        if self.unet is None:
            self.unet = DiffusionModelUNet(**self.config.unet_config).to(self.device)
            unet_params = sum(p.numel() for p in self.unet.parameters()) / 1e6
            print(f"âœ… UNetå‚æ•°é‡: {unet_params:.2f}M")
        
        # Inferer
        inferer = LatentDiffusionInferer(self.scheduler, scale_factor=scale_factor)
        
        cfg = self.config.diffusion_train
        
        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦
        optimizer = torch.optim.Adam(self.unet.parameters(), lr=cfg["lr"])
        scaler = GradScaler()
        
        # å­¦ä¹ ç‡warmup
        warmup_steps = cfg.get("warmup_steps", 0)
        total_steps = len(train_loader) * cfg["n_epochs"]
        
        def get_lr_multiplier(current_step):
            if warmup_steps > 0 and current_step < warmup_steps:
                return current_step / warmup_steps
            return 1.0
        
        accumulation_steps = self.config.optimization["gradient_accumulation_steps"]
        global_step = 0
        
        for epoch in range(cfg["n_epochs"]):
            self.unet.train()
            self.autoencoder.eval()
            
            epoch_loss = 0
            
            progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=100)
            progress_bar.set_description(f"Epoch {epoch+1}/{cfg['n_epochs']}")
            
            for step, batch in progress_bar:
                images = batch["image"].to(self.device)
                
                # æ›´æ–°å­¦ä¹ ç‡ï¼ˆwarmupï¼‰
                lr_mult = get_lr_multiplier(global_step)
                for param_group in optimizer.param_groups:
                    param_group['lr'] = cfg["lr"] * lr_mult
                
                with autocast(enabled=self.config.optimization["use_amp"]):
                    # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
                    z_mu, z_sigma = self.autoencoder.encode(images)
                    z = self.autoencoder.sampling(z_mu, z_sigma)
                    
                    # ç”Ÿæˆå™ªå£°å’Œtimesteps
                    noise = torch.randn_like(z).to(self.device)
                    timesteps = torch.randint(
                        0, self.scheduler.num_train_timesteps,
                        (z.shape[0],), device=self.device
                    ).long()
                    
                    # ä½¿ç”¨infereré¢„æµ‹å™ªå£°ï¼ˆä¸å®˜æ–¹æ•™ç¨‹ä¸€è‡´ï¼‰
                    noise_pred = inferer(
                        inputs=images,
                        diffusion_model=self.unet,
                        noise=noise,
                        timesteps=timesteps,
                        autoencoder_model=self.autoencoder
                    )
                    
                    loss = F.mse_loss(noise_pred.float(), noise.float())
                    loss = loss / accumulation_steps
                
                scaler.scale(loss).backward()
                
                # æ›´æ–°
                if (step + 1) % accumulation_steps == 0:
                    if self.config.optimization["max_grad_norm"]:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.unet.parameters(),
                            self.config.optimization["max_grad_norm"]
                        )
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad(set_to_none=True)
                    global_step += 1
                
                epoch_loss += loss.item() * accumulation_steps
                
                progress_bar.set_postfix({
                    "loss": f"{epoch_loss/(step+1):.4f}",
                    "mem": f"{torch.cuda.max_memory_allocated(self.device)/1024**3:.1f}GB"
                })
                
                if step % 50 == 0:
                    torch.cuda.empty_cache()
            
            avg_train_loss = epoch_loss / len(train_loader)
            self.history["diffusion_train_loss"].append(avg_train_loss)
            
            # éªŒè¯å’Œé‡‡æ ·
            if (epoch + 1) % cfg["val_interval"] == 0:
                val_loss = self._validate_diffusion(val_loader, inferer)
                self.history["diffusion_val_loss"].append(val_loss)
                print(f"\nğŸ“Š Epoch {epoch+1} - Val Loss: {val_loss:.4f}")
                
                # ä¿å­˜checkpoint
                self._save_checkpoint("diffusion", epoch, val_loss, scale_factor=scale_factor)
                
                # ç”Ÿæˆæ ·æœ¬
                self._generate_samples(inferer, epoch, num_samples=4)
            
            progress_bar.close()
        
        print("\nâœ… Diffusion Modelè®­ç»ƒå®Œæˆï¼")
        
        return scale_factor
    
    def _validate_autoencoder(self, val_loader):
        """éªŒè¯AutoencoderKL"""
        self.autoencoder.eval()
        val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                images = batch["image"].to(self.device)
                with autocast(enabled=True):
                    reconstruction, _, _ = self.autoencoder(images)
                    loss = F.l1_loss(reconstruction.float(), images.float())
                    val_loss += loss.item()
        
        self.autoencoder.train()
        return val_loss / len(val_loader)
    
    def _validate_diffusion(self, val_loader, inferer):
        """éªŒè¯Diffusionæ¨¡å‹"""
        self.unet.eval()
        val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                images = batch["image"].to(self.device)
                with autocast(enabled=True):
                    z_mu, z_sigma = self.autoencoder.encode(images)
                    z = self.autoencoder.sampling(z_mu, z_sigma)
                    
                    noise = torch.randn_like(z).to(self.device)
                    timesteps = torch.randint(
                        0, self.scheduler.num_train_timesteps,
                        (z.shape[0],), device=self.device
                    ).long()
                    
                    # ä½¿ç”¨infereré¢„æµ‹å™ªå£°ï¼ˆä¸å®˜æ–¹æ•™ç¨‹ä¸€è‡´ï¼‰
                    noise_pred = inferer(
                        inputs=images,
                        diffusion_model=self.unet,
                        noise=noise,
                        timesteps=timesteps,
                        autoencoder_model=self.autoencoder
                    )
                    
                    loss = F.mse_loss(noise_pred.float(), noise.float())
                    val_loss += loss.item()
        
        self.unet.train()
        return val_loss / len(val_loader)
    
    def _compute_scale_factor(self, train_loader):
        """è®¡ç®—scaling factor"""
        self.autoencoder.eval()
        
        with torch.no_grad():
            batch = next(iter(train_loader))
            images = batch["image"].to(self.device)
            with autocast(enabled=True):
                z = self.autoencoder.encode_stage_2_inputs(images)
        
        scale_factor = 1 / torch.std(z)
        self.autoencoder.train()
        return scale_factor.item()
    
    def _save_checkpoint(self, stage, epoch, loss, scale_factor=None):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            "epoch": epoch,
            "stage": stage,
            "loss": loss,
            "config": self.config.__dict__,
        }
        
        if stage == "autoencoder":
            checkpoint["autoencoder_state_dict"] = self.autoencoder.state_dict()
        elif stage == "diffusion":
            checkpoint["autoencoder_state_dict"] = self.autoencoder.state_dict()
            checkpoint["unet_state_dict"] = self.unet.state_dict()
            checkpoint["scale_factor"] = scale_factor
        
        filename = self.output_dir / "checkpoints" / f"{stage}_epoch_{epoch+1}.pth"
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ Checkpointå·²ä¿å­˜: {filename}")
    
    def _save_reconstruction_samples(self, original, reconstruction, epoch, prefix):
        """ä¿å­˜é‡å»ºæ ·æœ¬"""
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        
        for i in range(min(4, original.shape[0])):
            # åŸå›¾
            axes[0, i].imshow(original[i, 0].cpu().detach().numpy(), cmap='gray')
            axes[0, i].set_title(f"Original {i+1}")
            axes[0, i].axis('off')
            
            # é‡å»º
            axes[1, i].imshow(reconstruction[i, 0].cpu().detach().numpy(), cmap='gray')
            axes[1, i].set_title(f"Reconstructed {i+1}")
            axes[1, i].axis('off')
        
        plt.tight_layout()
        filename = self.output_dir / "samples" / f"{prefix}_reconstruction_epoch_{epoch+1}.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ–¼ï¸  é‡å»ºæ ·æœ¬å·²ä¿å­˜: {filename}")
    
    def _generate_samples(self, inferer, epoch, num_samples=4):
        """ç”Ÿæˆæ–°æ ·æœ¬ï¼ˆé€æ­¥å»å™ªé‡‡æ ·ï¼‰"""
        self.unet.eval()
        self.autoencoder.eval()
        
        fig, axes = plt.subplots(1, num_samples, figsize=(4*num_samples, 4))
        if num_samples == 1:
            axes = [axes]
        
        with torch.no_grad():
            for i in range(num_samples):
                # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒå™ªå£°å¼€å§‹
                noise = torch.randn(
                    (1, self.config.autoencoder_config["latent_channels"],
                     self.config.latent_size, self.config.latent_size)
                ).to(self.device)
                
                # è®¾ç½®é‡‡æ ·æ­¥æ•°
                self.scheduler.set_timesteps(num_inference_steps=1000)
                
                # ä½¿ç”¨inferer.sampleè¿›è¡Œé‡‡æ ·ï¼ˆä¸å®˜æ–¹æ•™ç¨‹ä¸€è‡´ï¼‰
                with autocast(enabled=True):
                    sample = inferer.sample(
                        input_noise=noise,
                        diffusion_model=self.unet,
                        scheduler=self.scheduler,
                        autoencoder_model=self.autoencoder
                    )
                
                axes[i].imshow(sample[0, 0].cpu().numpy(), cmap='gray')
                axes[i].set_title(f"Sample {i+1}")
                axes[i].axis('off')
        
        plt.tight_layout()
        filename = self.output_dir / "samples" / f"generated_epoch_{epoch+1}.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ¨ ç”Ÿæˆæ ·æœ¬å·²ä¿å­˜: {filename}")
        
        self.unet.train()
    
    def plot_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # AutoEncoder loss
        if self.history["autoencoder_train_loss"]:
            axes[0].plot(self.history["autoencoder_train_loss"], label="Train")
            if self.history["autoencoder_val_loss"]:
                val_epochs = np.linspace(
                    self.config.autoencoder_train["val_interval"],
                    len(self.history["autoencoder_train_loss"]),
                    len(self.history["autoencoder_val_loss"])
                )
                axes[0].plot(val_epochs, self.history["autoencoder_val_loss"], label="Val")
            axes[0].set_title("AutoEncoder Loss")
            axes[0].set_xlabel("Epoch")
            axes[0].set_ylabel("Loss")
            axes[0].legend()
            axes[0].grid(True)
        
        # Diffusion loss
        if self.history["diffusion_train_loss"]:
            axes[1].plot(self.history["diffusion_train_loss"], label="Train")
            if self.history["diffusion_val_loss"]:
                val_epochs = np.linspace(
                    self.config.diffusion_train["val_interval"],
                    len(self.history["diffusion_train_loss"]),
                    len(self.history["diffusion_val_loss"])
                )
                axes[1].plot(val_epochs, self.history["diffusion_val_loss"], label="Val")
            axes[1].set_title("Diffusion Loss")
            axes[1].set_xlabel("Epoch")
            axes[1].set_ylabel("Loss")
            axes[1].legend()
            axes[1].grid(True)
        
        plt.tight_layout()
        filename = self.output_dir / "training_history.png"
        plt.savefig(filename, dpi=150)
        plt.close()
        print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: {filename}")


# ============================================
# 4. ä¸»å‡½æ•°
# ============================================

def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒTIFFå †æ ˆçš„LDM")
    parser.add_argument("--tiff_path", type=str, required=True, help="TIFFå †æ ˆæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./output_ldm", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--image_size", type=int, default=1024, help="å›¾åƒå°ºå¯¸")
    parser.add_argument("--max_images", type=int, default=None, help="æœ€å¤šä½¿ç”¨å¤šå°‘å¼ å›¾åƒ")
    parser.add_argument("--batch_size", type=int, default=None, help="æ‰¹æ¬¡å¤§å°ï¼ˆNoneåˆ™è‡ªåŠ¨ï¼‰")
    parser.add_argument("--skip_autoencoder", action="store_true", help="è·³è¿‡AutoEncoderè®­ç»ƒ")
    parser.add_argument("--skip_diffusion", action="store_true", help="è·³è¿‡Diffusionè®­ç»ƒ")
    parser.add_argument("--autoencoder_checkpoint", type=str, default=None, help="AutoEncoder checkpointè·¯å¾„")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ï¼šcuda, cuda:0, cuda:1, cuda:2ç­‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_determinism(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    if "cuda" in args.device:
        if not torch.cuda.is_available():
            print("âŒ æœªæ£€æµ‹åˆ°CUDAï¼Œæœ¬è„šæœ¬éœ€è¦GPUè¿è¡Œï¼")
            return
        device = torch.device(args.device)
        # è·å–è®¾å¤‡ID
        device_id = 0 if args.device == "cuda" else int(args.device.split(":")[-1])
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {device} (GPU {device_id})")
        print(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(device_id).total_memory / 1024**3:.1f} GB")
    else:
        device = torch.device(args.device)
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    # åˆ›å»ºé…ç½®
    config = LDMConfig(image_size=args.image_size)
    if args.batch_size is not None:
        config.batch_size = args.batch_size
    config.print_summary()
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset = TiffStackDataset(args.tiff_path, transform=None, max_images=args.max_images)
    
    # æ ¹æ®æ•°æ®ç±»å‹è‡ªåŠ¨ç¡®å®šå½’ä¸€åŒ–èŒƒå›´
    sample_data = dataset.images[0]
    if sample_data.dtype == np.uint8:
        scale_max = 255.0
    elif sample_data.dtype == np.uint16:
        scale_max = 65535.0
    elif sample_data.dtype in [np.float32, np.float64]:
        scale_max = float(dataset.images.max())
    else:
        scale_max = float(dataset.images.max())
    
    print(f"ğŸ“Š æ•°æ®å½’ä¸€åŒ–èŒƒå›´: [0, {scale_max}] -> [0, 1]")
    
    # æ•°æ®å˜æ¢ - å¢å¼ºæ•°æ®å¢å¼ºä»¥æ‰©å……æ•°æ®é‡
    train_transforms = transforms.Compose([
        transforms.ScaleIntensityRanged(keys=["image"], a_min=0.0, a_max=scale_max, 
                                        b_min=0.0, b_max=1.0, clip=True),
        transforms.Resized(keys=["image"], spatial_size=[config.image_size, config.image_size]),  # ç¡®ä¿å°ºå¯¸æ­£ç¡®
        transforms.RandAffined(
            keys=["image"],
            rotate_range=[(-np.pi / 12, np.pi / 12), (-np.pi / 12, np.pi / 12)],  # âœ… å¢å¤§æ—‹è½¬èŒƒå›´
            translate_range=[(-50, 50), (-50, 50)],  # âœ… å¢å¤§å¹³ç§»èŒƒå›´
            scale_range=[(-0.15, 0.15), (-0.15, 0.15)],  # âœ… å¢å¤§ç¼©æ”¾èŒƒå›´
            spatial_size=[config.image_size, config.image_size],
            padding_mode="zeros",
            prob=0.9,  # âœ… æé«˜å¢å¼ºæ¦‚ç‡
        ),
        transforms.RandFlipd(keys=["image"], prob=0.5, spatial_axis=0),  # âœ… æ–°å¢ï¼šæ°´å¹³ç¿»è½¬
        transforms.RandFlipd(keys=["image"], prob=0.5, spatial_axis=1),  # âœ… æ–°å¢ï¼šå‚ç›´ç¿»è½¬
        transforms.RandGaussianNoised(keys=["image"], prob=0.3, mean=0.0, std=0.01),  # âœ… å¢åŠ æ¦‚ç‡
        transforms.RandAdjustContrastd(keys=["image"], prob=0.5, gamma=(0.7, 1.3)),  # âœ… å¢å¼ºå¯¹æ¯”åº¦èŒƒå›´
    ])
    
    val_transforms = transforms.Compose([
        transforms.ScaleIntensityRanged(keys=["image"], a_min=0.0, a_max=scale_max,
                                        b_min=0.0, b_max=1.0, clip=True),
        transforms.Resized(keys=["image"], spatial_size=[config.image_size, config.image_size]),
    ])
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(config.train_split * len(dataset))
    val_size = len(dataset) - train_size
    
    # éšæœºåˆ’åˆ†ç´¢å¼•
    indices = list(range(len(dataset)))
    np.random.seed(args.seed)
    np.random.shuffle(indices)
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    # è·å–å·²åŠ è½½çš„å›¾åƒæ•°ç»„
    all_images = dataset.images
    
    # åˆ›å»ºç‹¬ç«‹çš„æ•°æ®é›†ï¼ˆä½¿ç”¨images_arrayå‚æ•°é¿å…é‡å¤åŠ è½½TIFFï¼‰
    train_dataset = TiffStackDataset(
        images_array=all_images[train_indices],
        transform=train_transforms
    )
    val_dataset = TiffStackDataset(
        images_array=all_images[val_indices],
        transform=val_transforms
    )
    
    print(f"âœ… è®­ç»ƒé›†: {train_size} å¼ å›¾åƒ")
    print(f"âœ… éªŒè¯é›†: {val_size} å¼ å›¾åƒ")
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=True,
        persistent_workers=True,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True,
        persistent_workers=True,
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LDMTrainer(config, args.output_dir, device)
    
    # è®­ç»ƒAutoEncoder
    scale_factor = None
    if not args.skip_autoencoder:
        if args.autoencoder_checkpoint:
            print(f"ğŸ“¦ åŠ è½½AutoEncoder checkpoint: {args.autoencoder_checkpoint}")
            checkpoint = torch.load(args.autoencoder_checkpoint)
            trainer.autoencoder.load_state_dict(checkpoint["autoencoder_state_dict"])
            scale_factor = checkpoint.get("scale_factor")
        else:
            scale_factor = trainer.train_autoencoder(train_loader, val_loader)
    else:
        print("â­ï¸  è·³è¿‡AutoEncoderè®­ç»ƒ")
        if args.autoencoder_checkpoint:
            print(f"ğŸ“¦ åŠ è½½AutoEncoder checkpoint: {args.autoencoder_checkpoint}")
            checkpoint = torch.load(args.autoencoder_checkpoint)
            trainer.autoencoder.load_state_dict(checkpoint["autoencoder_state_dict"])
            scale_factor = checkpoint.get("scale_factor")
    
    # è®­ç»ƒDiffusion
    if not args.skip_diffusion:
        trainer.train_diffusion(train_loader, val_loader, scale_factor)
    else:
        print("â­ï¸  è·³è¿‡Diffusionè®­ç»ƒ")
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    trainer.plot_history()
    
    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   - checkpoints/: æ¨¡å‹checkpoint")
    print(f"   - samples/: ç”Ÿæˆæ ·æœ¬")
    print(f"   - training_history.png: è®­ç»ƒæ›²çº¿")
    print("="*60)


if __name__ == "__main__":
    main()

