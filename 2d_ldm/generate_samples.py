"""
ä½¿ç”¨è®­ç»ƒå¥½çš„LDMç”Ÿæˆæ–°æ ·æœ¬
æ”¯æŒæ‰¹é‡ç”Ÿæˆå’Œå¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•:
    python generate_samples.py --checkpoint ./output_ldm/checkpoints/diffusion_epoch_250.pth --num_samples 10
"""

import argparse
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm
from torch.cuda.amp import autocast

from generative.networks.nets import AutoencoderKL, DiffusionModelUNet
from generative.networks.schedulers import DDPMScheduler
from generative.inferers import LatentDiffusionInferer

try:
    import tifffile
    HAS_TIFFFILE = True
except ImportError:
    HAS_TIFFFILE = False
    print("âš ï¸  æœªå®‰è£…tifffileï¼Œæ— æ³•ä¿å­˜TIFFæ ¼å¼")


def load_model(checkpoint_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ“¦ åŠ è½½checkpoint: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # è·å–é…ç½®
    config = checkpoint.get("config", {})
    
    # é‡å»ºAutoencoderKLé…ç½®
    if "autoencoder_config" in config:
        ae_config = config["autoencoder_config"]
    else:
        # é»˜è®¤é…ç½®
        ae_config = {
            "spatial_dims": 2,
            "in_channels": 1,
            "out_channels": 1,
            "num_channels": (128, 256, 512),
            "latent_channels": 4,
            "num_res_blocks": 2,
            "attention_levels": (False, False, True),
            "with_encoder_nonlocal_attn": False,
            "with_decoder_nonlocal_attn": False,
        }
    
    # é‡å»ºUNeté…ç½®
    if "unet_config" in config:
        unet_config = config["unet_config"]
    else:
        unet_config = {
            "spatial_dims": 2,
            "in_channels": 4,
            "out_channels": 4,
            "num_res_blocks": 2,
            "num_channels": (128, 256, 512, 768),
            "attention_levels": (False, False, True, True),
            "num_head_channels": (0, 0, 512, 768),
        }
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ”§ åˆ›å»ºæ¨¡å‹...")
    autoencoder = AutoencoderKL(**ae_config).to(device)
    unet = DiffusionModelUNet(**unet_config).to(device)
    
    # åŠ è½½æƒé‡
    autoencoder.load_state_dict(checkpoint["autoencoder_state_dict"])
    unet.load_state_dict(checkpoint["unet_state_dict"])
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    autoencoder.eval()
    unet.eval()
    
    # è·å–scale_factor
    scale_factor = checkpoint.get("scale_factor", 1.0)
    
    # è·å–scheduleré…ç½®
    if "scheduler_config" in config:
        scheduler_config = config["scheduler_config"]
    else:
        scheduler_config = {
            "num_train_timesteps": 1000,
            "schedule": "scaled_linear_beta",
            "beta_start": 0.00085,
            "beta_end": 0.012,
        }
    
    scheduler = DDPMScheduler(**scheduler_config)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   - Scaling factor: {scale_factor:.4f}")
    print(f"   - Latent channels: {ae_config['latent_channels']}")
    
    # è®¡ç®—æ½œåœ¨ç©ºé—´å°ºå¯¸
    image_size = config.get("image_size", 1024)
    num_layers = len(ae_config["num_channels"])
    latent_size = image_size // (2 ** num_layers)
    
    return autoencoder, unet, scheduler, scale_factor, ae_config["latent_channels"], latent_size


def generate_samples(
    autoencoder,
    unet,
    scheduler,
    scale_factor,
    latent_channels,
    latent_size,
    device,
    num_samples=10,
    num_inference_steps=1000,
    save_intermediates=False,
    batch_size=1,
):
    """ç”Ÿæˆæ ·æœ¬"""
    
    inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)
    scheduler.set_timesteps(num_inference_steps=num_inference_steps)
    
    print(f"\nğŸ¨ ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬...")
    print(f"   - æ¨ç†æ­¥æ•°: {num_inference_steps}")
    print(f"   - æ½œåœ¨ç©ºé—´: {latent_size}Ã—{latent_size}Ã—{latent_channels}")
    
    all_samples = []
    all_intermediates = []
    
    num_batches = (num_samples + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for batch_idx in tqdm(range(num_batches), desc="ç”Ÿæˆä¸­"):
            current_batch_size = min(batch_size, num_samples - batch_idx * batch_size)
            
            # ä»å™ªå£°å¼€å§‹
            noise = torch.randn(
                (current_batch_size, latent_channels, latent_size, latent_size)
            ).to(device)
            
            with autocast(enabled=True):
                if save_intermediates:
                    samples, intermediates = inferer.sample(
                        input_noise=noise,
                        diffusion_model=unet,
                        scheduler=scheduler,
                        autoencoder_model=autoencoder,
                        save_intermediates=True,
                        intermediate_steps=num_inference_steps // 10,
                    )
                    all_intermediates.append(intermediates)
                else:
                    samples = inferer.sample(
                        input_noise=noise,
                        diffusion_model=unet,
                        scheduler=scheduler,
                        autoencoder_model=autoencoder,
                    )
            
            all_samples.append(samples.cpu())
    
    all_samples = torch.cat(all_samples, dim=0)
    
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼å½¢çŠ¶: {all_samples.shape}")
    
    if save_intermediates:
        return all_samples, all_intermediates
    return all_samples, None


def save_samples(samples, output_dir, prefix="sample"):
    """ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜æ ·æœ¬åˆ°: {output_dir}")
    
    # ä¿å­˜ä¸ºå•ç‹¬çš„å›¾åƒ
    for i, sample in enumerate(samples):
        img = sample[0].numpy()  # (H, W)
        
        # ä¿å­˜ä¸ºPNG
        plt.figure(figsize=(10, 10))
        plt.imshow(img, cmap='gray')
        plt.axis('off')
        plt.tight_layout(pad=0)
        png_path = output_dir / f"{prefix}_{i+1:04d}.png"
        plt.savefig(png_path, dpi=150, bbox_inches='tight', pad_inches=0)
        plt.close()
    
    print(f"âœ… ä¿å­˜äº† {len(samples)} ä¸ªPNGæ–‡ä»¶")
    
    # ä¿å­˜ä¸ºTIFFå †æ ˆ
    if HAS_TIFFFILE and len(samples) > 1:
        tiff_stack = np.stack([s[0].numpy() for s in samples])
        tiff_path = output_dir / f"{prefix}_stack.tif"
        tifffile.imwrite(tiff_path, tiff_stack)
        print(f"âœ… ä¿å­˜TIFFå †æ ˆ: {tiff_path}")
    
    # åˆ›å»ºç½‘æ ¼å¯è§†åŒ–
    num_samples = len(samples)
    ncols = min(5, num_samples)
    nrows = (num_samples + ncols - 1) // ncols
    
    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*3, nrows*3))
    if nrows == 1 and ncols == 1:
        axes = [[axes]]
    elif nrows == 1:
        axes = [axes]
    elif ncols == 1:
        axes = [[ax] for ax in axes]
    
    for idx in range(num_samples):
        row = idx // ncols
        col = idx % ncols
        axes[row][col].imshow(samples[idx, 0].numpy(), cmap='gray')
        axes[row][col].set_title(f"Sample {idx+1}")
        axes[row][col].axis('off')
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(num_samples, nrows * ncols):
        row = idx // ncols
        col = idx % ncols
        axes[row][col].axis('off')
    
    plt.tight_layout()
    grid_path = output_dir / f"{prefix}_grid.png"
    plt.savefig(grid_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… ä¿å­˜ç½‘æ ¼å›¾: {grid_path}")


def visualize_denoising_process(intermediates, output_path):
    """å¯è§†åŒ–å»å™ªè¿‡ç¨‹"""
    print(f"\nğŸ¬ å¯è§†åŒ–å»å™ªè¿‡ç¨‹...")
    
    # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
    if isinstance(intermediates, list):
        intermediates = intermediates[0]
    
    num_steps = len(intermediates)
    
    fig, axes = plt.subplots(1, num_steps, figsize=(num_steps*3, 3))
    if num_steps == 1:
        axes = [axes]
    
    for i, intermediate in enumerate(intermediates):
        img = intermediate[0, 0].cpu().numpy()
        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f"Step {i*100}")
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… å»å™ªè¿‡ç¨‹å·²ä¿å­˜: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨LDMç”Ÿæˆæ–°æ ·æœ¬")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./generated_samples", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--num_samples", type=int, default=10, help="ç”Ÿæˆæ ·æœ¬æ•°é‡")
    parser.add_argument("--num_inference_steps", type=int, default=1000, help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--save_intermediates", action="store_true", help="ä¿å­˜ä¸­é—´å»å™ªæ­¥éª¤")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # æ£€æŸ¥CUDA
    device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    autoencoder, unet, scheduler, scale_factor, latent_channels, latent_size = load_model(
        args.checkpoint, device
    )
    
    # ç”Ÿæˆæ ·æœ¬
    samples, intermediates = generate_samples(
        autoencoder,
        unet,
        scheduler,
        scale_factor,
        latent_channels,
        latent_size,
        device,
        num_samples=args.num_samples,
        num_inference_steps=args.num_inference_steps,
        save_intermediates=args.save_intermediates,
        batch_size=args.batch_size,
    )
    
    # ä¿å­˜æ ·æœ¬
    save_samples(samples, args.output_dir)
    
    # å¯è§†åŒ–å»å™ªè¿‡ç¨‹
    if args.save_intermediates and intermediates:
        denoising_path = Path(args.output_dir) / "denoising_process.png"
        visualize_denoising_process(intermediates, denoising_path)
    
    print("\n" + "="*60)
    print("ğŸ‰ ç”Ÿæˆå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   - sample_XXXX.png: å•ç‹¬çš„æ ·æœ¬")
    print(f"   - sample_grid.png: æ ·æœ¬ç½‘æ ¼")
    if HAS_TIFFFILE:
        print(f"   - sample_stack.tif: TIFFå †æ ˆ")
    if args.save_intermediates:
        print(f"   - denoising_process.png: å»å™ªè¿‡ç¨‹")
    print("="*60)


if __name__ == "__main__":
    main()

